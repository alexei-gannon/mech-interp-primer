<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="A brief primer on mechanistic interpretability for technically-minded people unfamiliar with the field â€” covering superposition, sparse autoencoders, transcoders, attribution graphs, and applications beyond AI safety.">
<meta property="og:title" content="A Technical Primer on Mechanistic Interpretability">
<meta property="og:description" content="A brief primer on mechanistic interpretability â€” covering superposition, sparse autoencoders, transcoders, attribution graphs, and applications beyond AI safety.">
<meta property="og:type" content="article">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Technical Primer on Mechanistic Interpretability">
<meta name="twitter:description" content="A brief primer on mechanistic interpretability for technically-minded people unfamiliar with the field.">
<title>A Technical Primer on Mechanistic Interpretability</title>
<style>
@import url('https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:ital,wght@0,300;0,400;0,500;0,600;1,300;1,400&family=IBM+Plex+Serif:ital,wght@0,300;0,400;0,500;0,600;1,300;1,400&display=swap');
*{margin:0;padding:0;box-sizing:border-box}
:root{
  --bg:#f5f0e8;--sf:#ece6da;--sfa:#e4ddd0;--bd:#d6cfc2;--bl:#e0d9cc;
  --tx:#1a1714;--dm:#40392f;--ft:#9e9486;--vf:#bfb6a8;
  --rd:#c4342d;--rl:#d4836a;--rk:#8b1a1a;
  --dk:#2a2420;--dkm:#3d3028;--wm:#b8564e;
  --blue:#4a6a8a;--gn:#5a7a52;--pu:#7a5a8a;
  --serif:'IBM Plex Serif',Georgia,'Times New Roman',serif;
  --mono:'IBM Plex Mono',monospace;
}
html{scroll-behavior:smooth}
body{background:var(--bg);color:var(--tx);font-family:var(--serif);line-height:1.85;overflow-x:hidden;-webkit-font-smoothing:antialiased}
.wrap{max-width:720px;margin:0 auto;padding:0 32px}

/* â”€â”€ Header â”€â”€ */
header{padding:96px 0 48px;border-bottom:1px solid var(--bd)}
header h1{font-family:var(--mono);font-weight:300;font-size:30px;letter-spacing:-0.5px;line-height:1.35;margin-bottom:20px;color:var(--dk)}
header .lead{font-size:15px;color:var(--dm);line-height:1.9;max-width:640px}

/* â”€â”€ Sections â”€â”€ */
section{padding:64px 0 48px;border-bottom:1px solid var(--bl)}
section:last-of-type{border-bottom:none;padding-bottom:96px}
h2{font-family:var(--mono);font-weight:400;font-size:11px;letter-spacing:3.5px;text-transform:uppercase;color:var(--rd);margin-bottom:28px}

/* â”€â”€ Body text â”€â”€ */
p{font-size:16px;color:var(--dm);margin-bottom:22px;line-height:1.9;letter-spacing:0.01em}
p:last-child{margin-bottom:0}
em{font-style:italic;color:var(--tx)}
strong{font-weight:600;color:var(--tx)}

/* â”€â”€ Blockquote â”€â”€ */
blockquote{border-left:2px solid var(--rd);padding:22px 28px;margin:28px 0;background:var(--sf);border-radius:0 6px 6px 0}
blockquote p{font-size:14.5px;color:var(--dm);margin-bottom:12px;line-height:1.85}
blockquote p:last-child{margin-bottom:0}
blockquote strong{color:var(--rd)}

/* â”€â”€ Figures & canvases â”€â”€ */
figure{margin:36px -12px;background:var(--sf);border:1px solid var(--bd);border-radius:8px;overflow:hidden}
.canvas-wrap{position:relative;width:100%;overflow:hidden;display:flex;justify-content:center;background:var(--sf)}
.canvas-wrap canvas{display:block;max-width:100%;height:auto}
figcaption{padding:12px 18px;font-family:var(--mono);font-size:11px;color:var(--ft);letter-spacing:0.3px;border-top:1px solid var(--bl);line-height:1.6}

/* â”€â”€ Mode buttons â”€â”€ */
.mode-row{display:flex;gap:6px;padding:12px 18px;border-bottom:1px solid var(--bl);flex-wrap:wrap}
.impl-panel{padding:20px 24px}
.impl-text p{font-size:14.5px;color:var(--dm);line-height:1.9;margin-bottom:0}
.mbtn{padding:6px 14px;border-radius:4px;font-family:var(--mono);font-size:10px;letter-spacing:1.2px;text-transform:uppercase;border:1px solid var(--bd);color:var(--ft);cursor:pointer;transition:all .25s;background:transparent}
.mbtn:hover{border-color:var(--ft)}
.mbtn.active{border-color:var(--rd);color:var(--rd);background:rgba(196,52,45,.06)}

/* â”€â”€ Footer â”€â”€ */
footer{padding:48px 0;text-align:center;font-family:var(--mono);font-size:10px;color:var(--vf);letter-spacing:1px}

/* â”€â”€ Footnotes â”€â”€ */
sup.fn{font-family:var(--mono);font-size:9px;color:var(--rd);margin-left:1px;font-weight:500;cursor:default;vertical-align:super;line-height:0}
ol.footnotes{list-style:decimal;padding-left:28px;margin:0}
ol.footnotes li{font-size:13px;color:var(--dm);line-height:1.75;margin-bottom:8px}
ol.footnotes li a{color:var(--rd);text-decoration:none;border-bottom:1px solid var(--rl);transition:border-color .2s}
ol.footnotes li a:hover{border-color:transparent}

/* â”€â”€ Glossary sidebar â”€â”€ */
.glossary{position:fixed;top:0;right:0;width:272px;height:100vh;background:var(--sf);border-left:1px solid var(--bd);overflow-y:auto;z-index:100;transform:translateX(100%);transition:transform .3s ease;-webkit-overflow-scrolling:touch}
.glossary.open{transform:translateX(0)}
.glossary-toggle{position:fixed;right:0;top:50%;transform:translateY(-50%);z-index:101;background:var(--sf);border:1px solid var(--bd);border-right:none;border-radius:6px 0 0 6px;padding:14px 9px;cursor:pointer;writing-mode:vertical-rl;font-family:var(--mono);font-size:10px;letter-spacing:2px;text-transform:uppercase;color:var(--rd);transition:background .2s, right .3s}
.glossary-toggle:hover{background:var(--sfa)}
.glossary.open~.glossary-toggle{right:272px}
.glossary-head{padding:20px 18px 14px;border-bottom:1px solid var(--bd);display:flex;justify-content:space-between;align-items:center}
.glossary-head span{font-family:var(--mono);font-size:10px;letter-spacing:2.5px;text-transform:uppercase;color:var(--rd);font-weight:500}
.glossary-close{background:none;border:none;font-size:18px;color:var(--ft);cursor:pointer;font-family:var(--mono);padding:0 4px}
.glossary-close:hover{color:var(--tx)}
.glossary-list{padding:8px 0}
.gl-item{border-bottom:1px solid var(--bl)}
.gl-term{display:block;width:100%;text-align:left;background:none;border:none;padding:11px 18px;font-family:var(--mono);font-size:11.5px;font-weight:500;color:var(--dk);cursor:pointer;transition:background .15s;line-height:1.4}
.gl-term:hover{background:var(--sfa)}
.gl-term::before{content:'â€º';display:inline-block;width:14px;color:var(--rd);font-size:14px;transition:transform .2s}
.gl-item.expanded .gl-term::before{transform:rotate(90deg)}
.gl-def{display:none;padding:2px 18px 14px 32px;font-family:var(--serif);font-size:12.5px;color:var(--dm);line-height:1.75}
.gl-item.expanded .gl-def{display:block}
.gl-section{padding:8px 18px 4px;font-family:var(--mono);font-size:8px;letter-spacing:2px;text-transform:uppercase;color:var(--ft);font-weight:500;margin-top:6px}

@media(max-width:600px){
  header h1{font-size:24px}
  .wrap{padding:0 20px}
  section{padding:48px 0 36px}
  p{font-size:15px}
  .glossary{width:min(300px,85vw)}
  .glossary.open~.glossary-toggle{right:min(300px,85vw)}
}
</style>
</head>
<body>
<div class="wrap">

<header>
  <h1>A Technical Primer on<br>Mechanistic Interpretability</h1>
  <p class="lead">A brief primer for technically-minded people unfamiliar with the discipline of mechanistic interpretabilityâ€”a field of research developing a causal understanding of the internal mechanisms of "black box" artificial neural networks. This primer includes the origin of the field, a history of transformer interpretability as applied to LLMs, and a few examples of how these methods may be useful to scientific research beyond AI Safety. In writing this, I have liberally simplified and lightly stolen from the extensive body of research on this topic; please read the sources I reference and beyond.</p>
</header>

<section>
  <h2>Motivation &amp; Background</h2>
  <p>On the one hand, I write this as a neuroscientist who believes the techniques developed by mechanistic interpretability have an underappreciated potential for extracting scientific facts about the world from machine learning modelsâ€”especially in the study of biological brains. On the other hand, I find mechanistic interpretability to be a tool of great importance for people involved in AI policy and governance. From both perspectives, an influx of interest in mechanistic interpretability from experts outside of the field may be broadly useful for AI safety.</p>
  <figure>
    <div class="mode-row" id="impl-modes">
      <button class="mbtn active" data-mode="science">Scientific Research</button>
      <button class="mbtn" data-mode="neuro">Neuroscience</button>
      <button class="mbtn" data-mode="audit">Auditing</button>
      <button class="mbtn" data-mode="reg">Regulatory Compliance</button>
    </div>
    <div class="impl-panel" id="impl-panel">
      <div class="impl-text" data-mode="science" style="display:block"><p>Large machine learning models like AlphaFold and the ESM Series have proven to be incredibly effective at predicting three-dimensional protein structures from amino acid sequences. Recent work across multiple labs<sup class="fn">1,2</sup> has begun to apply the methods of mechanistic interpretability to biology foundation models, identifying what undiscovered biological mechanisms these models have learned. Such investigations into the inner working of foundation models trained to complete other medical tasks, such as Alzheimer's detection, have identified a novel class of biomarkers<sup class="fn">3</sup>. This paradigm can be applied across scientific disciplines, uncovering facts about the world from black-box models.</p></div>
      <div class="impl-text" data-mode="neuro" style="display:none"><p>The most natural discipline to gain from developments in mechanistic interpretability is neuroscience. Tools developed to find interpretable features in artificial neural networks should transfer directly into the study of biological neural networks. The main bottleneck is how much less information one gets when recording from biological neurons versus an entire language model. This could be partially addressed through the increasing density of neural recordings, but domain-specific modifications are likely necessary.</p></div>
      <div class="impl-text" data-mode="audit" style="display:none"><p>More broadly, interpretability can be useful to audit in-deployment models for regulatory compliance. If a model is assigned to make decisions about an important task in law, medicine, or finance, it is important that there exists a way to verify that the model is not engaging in misaligned behaviors such as discrimination.</p></div>
      <div class="impl-text" data-mode="reg" style="display:none"><p>Furthermore, if it is desirable to implement laws that monitor or restrict certain types of AI researchâ€”preventing a model from being trained on Gain-of-Function research, maintaining a ratio of compute allocated to model capability and safety research, or controlling the development of frontier models through an international accordâ€”robust methods for interpretability may be necessary to verify compliance.</p></div>
    </div>
  </figure>
  <p>Therefore, I direct this piece towards a generally technical audience. I do not define my terms in complete formal mathematics for the sake of accessibilityâ€”please read the sources I reference for this type of discussion. My goal is to foster intuition about what types of questions this field is interested in asking, what methods they use and why, and what outcomes have guided the progression of this field over the past few years.</p>
</section>

<section>
  <h2>Interpreting Neural Networks</h2>
  <p>In 2020, OpenAI's interpretability team led by Chris Olah published <em>Zoom In: An Introduction to Circuits<sup class="fn">4</sup></em>. This paper laid out three speculative claims about neural networks that provide a foundation for mechanistic interpretability:</p>
  <blockquote>
    <p><strong>Claim 1: Features</strong> â€” Features are the fundamental unit of neural networks. They correspond to directions. These features can be rigorously studied and understood.</p>
    <p><strong>Claim 2: Circuits</strong> â€” Features are connected by weights, forming circuits. These circuits can also be rigorously studied and understood.</p>
    <p><strong>Claim 3: Universality</strong> â€” Analogous features and circuits form across models and tasks.</p>
  </blockquote>
  <p>This paper analyzed what images maximally activate individual neurons of a computer vision model to understand their functionâ€”much like how one can study the tuning curve of a biological neuron. The researchers identified some neurons that cleanly respond to individual features, like the orientation of a curve. Circuits construct complex features through a weighted combination of simpler features.</p>
  <p>Unfortunately, neural networks are not reducible to interpretable neurons. While some circuits construct complex features from natural-seeming primitives like "windows + car body + wheels = car," polysemantic neurons combine multiple complex features under superposition.</p>
  <figure>
    <div class="canvas-wrap"><canvas id="cv-circuits" width="724" height="260"></canvas></div>
    <figcaption>Left: Monosemantic neurons form an interpretable "car" circuit. Right: A polysemantic neuron mixes unrelated features â€” superposition.</figcaption>
  </figure>
  <p>Olah and others went on to investigate superposition at Anthropic in 2022 in <em>Toy Models of Superposition<sup class="fn">5</sup></em>. Here, they identified superposition as a natural way for neural networks to represent more features than dimensions.</p>
  <p>As an example, imagine a layer of a network has three neurons. If the optimal representation of features at this layer is three or less, then each neuron can represent one feature. You can imagine a 3-dimensional grid where each axis represents both a feature and a neuronâ€”each feature is orthogonal to each other. However, if it is optimal to store more than three features, then each other feature must be represented in a non-orthogonal direction within 3D space. While this should imply interference between features, if features are sparse, or rarely active, the network can avoid a loss in performance.</p>
  <figure>
    <div class="mode-row" id="sup-modes">
      <button class="mbtn active" data-n="3">3 features</button>
      <button class="mbtn" data-n="5">5 features</button>
      <button class="mbtn" data-n="8">8 features</button>
    </div>
    <div class="canvas-wrap"><canvas id="cv-super" width="724" height="320"></canvas></div>
    <figcaption>A 3-neuron network. Toggle to see how adding features forces non-orthogonal, superposed representations.</figcaption>
  </figure>
  <p>This presents a generalized problem for interpretability. If superposition is a predictable behavior for neural networks, then features will not simply fall out of analyzing the function of individual neurons. The field has developed two ways to respond to this problem. The first is to develop methods for feature extraction out of compressed representations. The second is to design interpretable architectures that disincentivize superposition. To have a concrete example of how these approaches have matured, let us focus on the interpretability of transformers, the field's primary area of interest.</p>
</section>

<section>
  <h2>Transformer Interpretability</h2>
  <p>In <em>A Mathematical Framework for Transformer Circuits<sup class="fn">6</sup></em>, a team led by Chris Olah at Anthropic including Nelson Elhage, Neel Nanda, and Catherine Olsson among others examined transformers of two layers and less to derive some general principles of their internal mechanisms. Transformers are the fundamental attention-based architecture behind every major LLM among others. Below is the anatomy of one layer of a transformer which takes in a tokenâ€”a small string in the fundamental vocabulary of the modelâ€”and outputs logits for the probability of the next token. The line running horizontally from "embed" to "unembed" representing the information carried between the input token and output logit is called the residual stream, a linear high-dimensional vector space.</p>
  <figure>
    <div class="canvas-wrap"><canvas id="cv-transformer" width="724" height="190"></canvas></div>
    <figcaption>One layer of a transformer. The residual stream carries information; attention and MLP blocks read from and write to it.</figcaption>
  </figure>
  <p>You can think of an LLM as stacking up layers of a transformer to apply more and more operations onto the content of the residual stream. On each layer, attention heads direct how much each token attends to each other token, reading from and adding onto the residual stream. Then, a shallow neural network called a multi-layer perceptron (MLP) takes the residual stream as input, applies an activation function, then adds onto the stream again. In short, attention heads move information between positions, then an MLP applies some computation on that information.</p>
  <p>The importance of this paper comes from the discovery of an interpretable circuit formed by the composition of attention heads. Specifically, the researchers identified the algorithm through which a two-layer attention-only transformer predicts patterns of the form: see [AB], see [A] â†’ predict [B]. On the first attention layer, some heads attend to the content of the token immediately before a given token, so "[A] precedes [B]" is stored in the residual stream. Making use of this, the second layer induction head attends from the current [A] to the [B] that followed the previous [A], and copies the value [B] to predict the next token.</p>
  <figure>
    <div class="canvas-wrap"><canvas id="cv-induction" width="724" height="270"></canvas></div>
    <figcaption>Induction head: Layer 1 stores "A precedes B"; Layer 2 finds the pattern and predicts B.</figcaption>
  </figure>
  <p>This was a proof-of-concept that trained models can implement baroque yet interpretable algorithmsâ€”a guiding light in an era of mechanistic interpretability that sought to completely reverse-engineer toy models. Nanda et al. identified that small transformers<sup class="fn">7</sup> use Fourier transforms to implement modular addition. Li et al.<sup class="fn">8</sup> could recover the board state from an 8 layer model trained to play Othello. Wang et al. recovered a circuit<sup class="fn">9</sup> for the identification of indirect objects in GPT-2 small. These are each quite elegant papers worth a closer read.</p>
  <p>At this point, researchers began to question the utility of this approach. It had become clear that LLMs are large, dense networks of uninterpretable neurons with complex interpretable functions buried underneath complexity. Identifying baroque algorithms an LLM can implement at each layer established the promise of this field, but repeating this type of work for every possible computation is infeasible. This problem incentivized a new goal for interpretability: the automatic detection of features at scale.</p>
</section>

<section>
  <h2>Dictionary Learning</h2>
  <p>Multiple teams began to investigate the efficacy of sparse autoencoders (SAEs). An SAE is a wide single-layer neural network that is tasked with learning sparse representations that can reproduce a layer of neural activations. This is one attempt at dictionary learning: the decomposition of distributed representations of a neural network into many interpretable features.</p>
  <figure>
    <div class="canvas-wrap"><canvas id="cv-sae" width="724" height="340"></canvas></div>
    <figcaption>A sparse autoencoder decomposes dense MLP activations into many sparse, interpretable features, then reconstructs the original.</figcaption>
  </figure>
  <p>While Bricken et al. at Anthropic studied<sup class="fn">10</sup> SAEs in the MLPs of one-layer transformers, Cunningham et al. at Eleuther AI researched<sup class="fn">11</sup> SAEs in the residual stream of GPT-2 size models, which Templeton et al. at Anthropic scaled up<sup class="fn">12</sup> to a complete dictionary of features of Claude 3 Sonnet. In all cases, researchers successfully identified interpretable features in an unsupervised mannerâ€”albeit the goal of accurately reducing a frontier model into a dictionary of features remained out of reach.</p>
  <p>In the pursuit of this goal, it was discovered that identified features, among other directions in activation space, could be used to direct the behavior of LLMsâ€”a process called activation steering. This opened up a new approach to mechanistic interpretability: even if we could not learn the entire model, we could learn what matters to succeed at the task at hand.</p>
</section>

<section>
  <h2>Representation Engineering</h2>
  <p>Identifying and activating directions within the model to control behavior is also referred to as representation engineering, another branch of research that began to converge with mechanistic interpretability at this time. This term was coined by Zou et al. at the Center for AI Safety<sup class="fn">13</sup>, who made use of the linear representation of features to identify and manipulate directions in activation space that induce behaviors like honesty or power-seeking. While one could target features identified by SAEs, the simplest way to identify a useful axis for manipulation is by constructing contrastive pairs: run prompts that elicit honesty, run prompts that elicit dishonesty, and calculate an honesty axis by subtracting the means of honest activations from dishonest activations.</p>
  <figure>
    <div class="canvas-wrap"><canvas id="cv-repe" width="724" height="360"></canvas></div>
    <figcaption>Representation engineering: subtract dishonest from honest activations to find a steering direction. Add it to steer behavior.</figcaption>
  </figure>
  <p>Representation engineering has been effective at eliciting truthful answers and identifying axes that mediate the refusal of harmful requests or model persona<sup class="fn">14,15,16</sup>. That said, the development of a useful axis is a subjective task which may not generalize out of distribution or even extend outside the behavior of interest. For example, when trying to mitigate social bias<sup class="fn">17</sup>, Anthropic found that steering features related to gender bias produced off-target effects on age bias.</p>
  <p>While this approach is not identical to mechanistic interpretability's goal of complete reverse engineering, it does provide a clear use-case for the methods developed by mechanistic interpretability along with an avenue to make causal claims between model internals and model behavior. <em>A Pragmatic Vision for Interpretability<sup class="fn">18</sup></em> argues that mechanistic interpretability research should choose the level of analysis most relevant for the safety-relevant task at hand, erring towards method minimalism.</p>
  <p>In this veinâ€”while SAEs are still useful to discover new features and analyze the geometry of a representationâ€”the field has embraced simpler methods to identify supervised features of interest. When trying to monitor an abstract feature like "harmful user intent" outside of the training distribution, Google DeepMind identified that SAEs were outperformed by linear probes<sup class="fn">19</sup>, a linear classifier/regressor trained on a layer of model activations. This provides a computationally cheap and interpretable way to monitor internals given a labeled dataset.</p>
</section>

<section>
  <h2>Transcoders</h2>
  <p>While the methods above are useful to operate on representations, they don't explain how the model computes. If we detect that the model is about to engage in deception at some layer, both scientific curiosity and pragmatism may lead us to ask how the processing of prior inputs led to that behaviorâ€”especially if activation steering is insufficient to alter our behavior of interest. To do this, we cannot simply learn the representation within an MLP; we must learn the transformation between the inputs of an MLP to its outputs. This is the purpose of a transcoder, a wide MLP that is trained to sparsely approximate the function of the MLP<sup class="fn">20</sup>, input-to-output, in a trained LLM.</p>
  <p>The optimal structure of a transcoder is an open question. The original approach trains one transcoder per layer. A skip transcoder<sup class="fn">21</sup> adds an affine skip connection to separate the linear component of the MLP from the nonlinear features. Cross-layer transcoders write to the MLP outputs<sup class="fn">22</sup> of all future layers, enabling clean attribution across the entire model at the expense of extra compute.</p>
  <p>Furthermore, one can measure how attention moves information between interpretable features through QK attribution. This is done by decomposing attention scores into sums of feature-pair dot products<sup class="fn">23</sup> between query and key positions. Together, transcoders and QK attribution make nearly the full forward pass interpretable.</p>
  <figure>
    <div class="mode-row" id="tc-modes">
      <button class="mbtn active" data-mode="plt">Per-Layer</button>
      <button class="mbtn" data-mode="clt">Cross-Layer</button>
      <button class="mbtn" data-mode="qk">CLT + QK Attribution</button>
    </div>
    <div class="canvas-wrap"><canvas id="cv-trans" width="724" height="400"></canvas></div>
    <figcaption>Left: Transformer with transcoder replacement. Right: Attribution graph â€” each method reveals more structure.</figcaption>
  </figure>
  <p>We can use these methods to create attribution graphs that trace a computation throughout a model. This can be used to understand reasoning, diagnose reasoning errors, or understand jailbreaks<sup class="fn">24,25,26</sup> in a causal manner. That said, it is important to remember that the replacement model is still an approximation, and not every prompt is neatly explainable.</p>
</section>

<section>
  <h2>Natural Language Explanation</h2>
  <p>For a more top-level explanation, these principles allow researchers to train a language model to explain the internals of another model. Caden Juang et al. at Stanford developed LatentQA<sup class="fn">27</sup>, which finetuned a decoder LLM to answer specific domains of questions about a target LLM from the activations of the target plus the input prompt. For example, this activation explainer can read what persona a model has been told to adopt via an otherwise hidden prompt. Furthermore, given this model has learned how to translate directions in activation space into natural language, it can also calculate a loss based on natural language prompts like "be an unbiased person" that can be backpropagated to the target model's activations to change model behavior during inference. Chen et al. at Anthropic generalized this framework through diversified training to develop activation oracles that can answer natural language queries<sup class="fn">28</sup> about model activity beyond the domain of their training distribution.</p>
  <p>Relatedly, researchers at Transluce found that models trained<sup class="fn">29</sup> for LatentQA-type tasks work best when the decoder LLM is generated by training the target LLM to explain its own activations. In combination with evidence from Lindsey et al. at Anthropic that models can detect external manipulations<sup class="fn">30</sup> of internal activations above chance, this provides preliminary evidence that frontier models have some capacity for introspection. It remains an open question how training models to perform introspection affects model capabilities.</p>
  <figure>
    <div class="mode-row" id="nle-modes">
      <button class="mbtn active" data-mode="latentqa">LatentQA</button>
      <button class="mbtn" data-mode="ao">Activation Oracle</button>
      <button class="mbtn" data-mode="intro">Introspection</button>
    </div>
    <div class="canvas-wrap"><canvas id="cv-nle" width="724" height="500"></canvas></div>
    <figcaption>LatentQA trains a decoder to read activations; activation oracles generalize via diverse training; introspection shows models explain themselves best.</figcaption>
  </figure>
</section>

<section>
  <h2>Training on Mechanistic Interpretability</h2>
  <p>In the field of AI Safety, training models on the measures we use for mechanistic interpretability has been called "The Most Forbidden Technique<sup class="fn">31</sup>." If we use a probe to detect a representation of some ideal feature like honesty, then training the model to activate said probe could merely train the model to represent honesty while learning some less interpretable way to engage in dishonest behaviorâ€”making the probe useless.</p>
  <p>That said, research may uncover useful and safe ways to train on mechanistic interpretabilityâ€”transforming this area of study from a taboo into a series of complex engineering questions. The main area of research thus far involves incorporating linear probes into training, as linear probes would not dramatically increase the amount of compute required for training. Concerning model honesty, Betley et al. at Anthropic found that maintaining honest behavior<sup class="fn">32</sup> when incorporating lie detectors into training required careful choices about probe design and training hyperparameters. From a feature-based perspective, Chung et al. demonstrated that ablation of misaligned features<sup class="fn">33</sup> during fine-tuning can reduce emergent misalignment without degrading performance. Goodfire has recently found success in using a linear probe that discerns between activations underlying factually true and false outputs to train an LLM against hallucination from the outset<sup class="fn">34</sup>.</p>
  <p>It is unclear what the trade-off between capabilities and safety will be in this domain on both practical and methodological dimensions. Neel Nanda of Google DeepMind has suggested<sup class="fn">35</sup> that integrating these techniques into frontier model training stacks would be such a pain that it is safe to ask empirical questions about what this approach might do long-term with minimal imminent risk. Thomas McGrath of Goodfire suggests training<sup class="fn">36</sup> on one set of methods and testing safety with another set of methods could mitigate the chance that models learn to obfuscate their internal representations. This is an unresolved question of active debate and research.</p>
</section>

<section>
  <h2>Conclusion</h2>
  <p>The capabilities presented by complex computational systems seem to increase more quickly every yearâ€”it is essential to understand how they function. At the core of these systems are artificial neural networks which represent features in superposition, making individual neurons uninterpretable. In the face of this challenge, the field has developed tools to extract interpretable features, trace computation through a model, explain behavior in natural language, and shape the development of new models.</p>
  <p>Mechanistic interpretability is still early. Attribution graphs are approximations. SAE features don't perfectly reconstruct the model they decompose. Training on interpretability signals raises adversarial concerns that are not yet resolved. The field has produced elegant results on toy models and increasingly compelling results on frontier models, but a complete mechanistic understanding of a system as large as a modern LLM remains out of reach. Whether this is a temporary bottleneck or a fundamental limitation is an open question.</p>
  <p>That said, the trajectory is promising enough to warrant broad attention. For those in the sciences, application of these methods to biology foundation models is already producing novel discoveries. For those in policy, the ability to audit a model's internal reasoning is a prerequisite for meaningful regulation of AI systems making consequential decisions. The methods described here are the best candidates we haveâ€”so far.</p>
  <p>I have tried to present this material at a level of detail that builds intuition without requiring the reader to have followed the field from the beginning. If I have succeeded, you should now be able to engage with the primary sources with some fluency; I encourage you to do so. The field is moving fast, and the papers referenced throughout this piece are a good starting point.</p>
</section>


<section>
  <h2>References</h2>
  <ol class="footnotes">
    <li id="fn-1"><a href="https://openreview.net/forum?id=zdOGBRQEbz" target="_blank" rel="noopener">Adams et al., "From Mechanistic Interpretability to Mechanistic Biology," ICML 2025.</a></li>
    <li id="fn-2"><a href="https://www.biorxiv.org/content/10.1101/2024.11.14.623630v1" target="_blank" rel="noopener">Simon &amp; Zou, "InterPLM: Discovering Interpretable Features in Protein Language Models," Nature Methods 2025.</a></li>
    <li id="fn-3"><a href="https://www.goodfire.ai/research/interpretability-for-alzheimers-detection" target="_blank" rel="noopener">Goodfire, "Interpretability for Alzheimer's Detection."</a></li>
    <li id="fn-4"><a href="https://distill.pub/2020/circuits/zoom-in" target="_blank" rel="noopener">Olah et al., "Zoom In: An Introduction to Circuits," Distill, 2020.</a></li>
    <li id="fn-5"><a href="https://transformer-circuits.pub/2022/toy_model/index.html" target="_blank" rel="noopener">Elhage et al., "Toy Models of Superposition," Transformer Circuits, 2022.</a></li>
    <li id="fn-6"><a href="https://transformer-circuits.pub/2021/framework/index.html" target="_blank" rel="noopener">Elhage et al., "A Mathematical Framework for Transformer Circuits," Transformer Circuits, 2021.</a></li>
    <li id="fn-7"><a href="https://arxiv.org/pdf/2301.05217" target="_blank" rel="noopener">Nanda et al., "Progress Measures for Grokking via Mechanistic Interpretability," ICLR 2023.</a></li>
    <li id="fn-8"><a href="https://arxiv.org/pdf/2210.13382" target="_blank" rel="noopener">Li et al., "Emergent World Representations," ICLR 2023.</a></li>
    <li id="fn-9"><a href="https://arxiv.org/pdf/2211.00593" target="_blank" rel="noopener">Wang et al., "Interpretability in the Wild: IOI in GPT-2 Small," ICLR 2023.</a></li>
    <li id="fn-10"><a href="https://transformer-circuits.pub/2023/monosemantic-features" target="_blank" rel="noopener">Bricken et al., "Towards Monosemanticity," Transformer Circuits, 2023.</a></li>
    <li id="fn-11"><a href="https://arxiv.org/pdf/2309.08600" target="_blank" rel="noopener">Cunningham et al., "Sparse Autoencoders Find Highly Interpretable Features in Language Models," ICLR 2024.</a></li>
    <li id="fn-12"><a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/" target="_blank" rel="noopener">Templeton et al., "Scaling Monosemanticity," Transformer Circuits, 2024.</a></li>
    <li id="fn-13"><a href="https://arxiv.org/abs/2310.01405" target="_blank" rel="noopener">Zou et al., "Representation Engineering," ICML 2024.</a></li>
    <li id="fn-14"><a href="https://arxiv.org/pdf/2306.03341" target="_blank" rel="noopener">Li et al., "Inference-Time Intervention: Eliciting Truthful Answers from a Language Model," NeurIPS 2023.</a></li>
    <li id="fn-15"><a href="https://arxiv.org/pdf/2406.11717" target="_blank" rel="noopener">Arditi et al., "Refusal in Language Models Is Mediated by a Single Direction," 2024.</a></li>
    <li id="fn-16"><a href="https://www.anthropic.com/research/assistant-axis" target="_blank" rel="noopener">Anthropic, "The Assistant Axis."</a></li>
    <li id="fn-17"><a href="https://www.anthropic.com/research/evaluating-feature-steering" target="_blank" rel="noopener">Anthropic, "Evaluating Feature Steering."</a></li>
    <li id="fn-18"><a href="https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability" target="_blank" rel="noopener">"A Pragmatic Vision for Interpretability," Alignment Forum.</a></li>
    <li id="fn-19"><a href="https://deepmindsafetyresearch.medium.com/negative-results-for-sparse-autoencoders-on-downstream-tasks-and-deprioritising-sae-research-6cadcfc125b9" target="_blank" rel="noopener">Google DeepMind, "Negative Results for Sparse Autoencoders on Downstream Tasks," 2025.</a></li>
    <li id="fn-20"><a href="https://arxiv.org/pdf/2406.11944" target="_blank" rel="noopener">Dunefsky et al., "Transcoders Find Interpretable LLM Feature Circuits," 2024.</a></li>
    <li id="fn-21"><a href="https://arxiv.org/pdf/2501.18823" target="_blank" rel="noopener">Lindsey &amp; Batson, "Skip Transcoders," 2025.</a></li>
    <li id="fn-22"><a href="https://transformer-circuits.pub/2025/attribution-graphs/methods.html" target="_blank" rel="noopener">Anthropic, "Circuit Tracing: Attribution Graphs Methods," Transformer Circuits, 2025.</a></li>
    <li id="fn-23"><a href="https://transformer-circuits.pub/2025/attention-qk/index.html" target="_blank" rel="noopener">Anthropic, "Attention QK Attribution," Transformer Circuits, 2025.</a></li>
    <li id="fn-24"><a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-misaligned" target="_blank" rel="noopener">Anthropic, "Circuit Tracing: Biology of a Large Language Model," 2025.</a></li>
    <li id="fn-25"><a href="https://arxiv.org/pdf/2510.09312" target="_blank" rel="noopener">Riveland, "Using Attribution Graphs to Diagnose Reasoning Errors," 2025.</a></li>
    <li id="fn-26"><a href="https://labs.zenity.io/p/interpreting-jailbreaks-and-prompt-injections-with-attribution-graphs" target="_blank" rel="noopener">Zenity, "Interpreting Jailbreaks with Attribution Graphs," 2025.</a></li>
    <li id="fn-27"><a href="https://arxiv.org/pdf/2412.08686" target="_blank" rel="noopener">Juang et al., "LatentQA: Teaching LLMs to Decode Activations Into Natural Language," 2024.</a></li>
    <li id="fn-28"><a href="https://alignment.anthropic.com/2025/activation-oracles/" target="_blank" rel="noopener">Chen et al., "Activation Oracles," Anthropic Alignment, 2025.</a></li>
    <li id="fn-29"><a href="https://transluce.org/self-explanations" target="_blank" rel="noopener">Transluce, "Self-Explanations."</a></li>
    <li id="fn-30"><a href="https://transformer-circuits.pub/2025/introspection/index.html" target="_blank" rel="noopener">Lindsey et al., "Emergent Introspective Awareness," Transformer Circuits, 2025.</a></li>
    <li id="fn-31"><a href="https://www.lesswrong.com/posts/mpmsK8KKysgSKDm2T/the-most-forbidden-technique" target="_blank" rel="noopener">"The Most Forbidden Technique," LessWrong.</a></li>
    <li id="fn-32"><a href="https://arxiv.org/pdf/2505.13787" target="_blank" rel="noopener">Betley et al., "Incorporating Lie Detectors into Training," 2025.</a></li>
    <li id="fn-33"><a href="https://arxiv.org/pdf/2507.16795" target="_blank" rel="noopener">Chung et al., "Concept Ablation Fine-Tuning," 2025.</a></li>
    <li id="fn-34"><a href="https://www.goodfire.ai/research/rlfr#probe-pipeline" target="_blank" rel="noopener">Goodfire, "Reinforcement Learning from Representations," 2026.</a></li>
    <li id="fn-35"><a href="https://www.alignmentforum.org/posts/G9HdpyREaCbFJjKu5/it-is-reasonable-to-research-how-to-use-model-internals-in" target="_blank" rel="noopener">Nanda, "It Is Reasonable to Research How to Use Model Internals in Training," Alignment Forum.</a></li>
    <li id="fn-36"><a href="https://www.goodfire.ai/blog/intentional-design" target="_blank" rel="noopener">Goodfire, "Intentional Design."</a></li>
  </ol>
</section>

<footer>A Technical Primer on Mechanistic Interpretability</footer>
</div>

<script>
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// IMPLICATIONS TAB SWITCHER
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
(function(){
  const row=document.getElementById('impl-modes');
  if(!row)return;
  row.addEventListener('click',function(e){
    if(!e.target.classList.contains('mbtn'))return;
    row.querySelectorAll('.mbtn').forEach(b=>b.classList.remove('active'));
    e.target.classList.add('active');
    const mode=e.target.dataset.mode;
    document.querySelectorAll('#impl-panel .impl-text').forEach(d=>{
      d.style.display=d.dataset.mode===mode?'block':'none';
    });
  });
})();

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// CANVAS SYSTEM: Fixed logical coords, CSS scales
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
const C={bg:'#f5f0e8',sf:'#ece6da',sfa:'#e4ddd0',bd:'#d6cfc2',bl:'#e0d9cc',tx:'#1a1714',dm:'#6b6359',ft:'#9e9486',vf:'#bfb6a8',rd:'#c4342d',rl:'#d4836a',rk:'#8b1a1a',dk:'#2a2420',dkm:'#3d3028',wm:'#b8564e',blue:'#4a6a8a',gn:'#5a7a52',pu:'#7a5a8a'};
const dpr=window.devicePixelRatio||1;
let t=0;

// Each canvas has fixed logical width/height in HTML attributes.
// We scale the backing store by dpr for sharpness.
// CSS width:100% + height:auto preserves aspect ratio.
function setup(id){
  const cv=document.getElementById(id);
  const lw=parseInt(cv.getAttribute('width'));
  const lh=parseInt(cv.getAttribute('height'));
  cv.width=lw*dpr; cv.height=lh*dpr;
  // CSS auto-scales via width:100%;height:auto
  const ctx=cv.getContext('2d');
  ctx.scale(dpr,dpr);
  return{cv,ctx,w:lw,h:lh};
}

function txt(c,s,x,y,sz,col,al,wt){
  c.font=(wt?wt+' ':'')+sz+'px "IBM Plex Mono",monospace';
  c.fillStyle=col;c.textAlign=al||'center';c.textBaseline='middle';c.fillText(s,x,y);
}
function rr(c,x,y,w,h,r,f,s,lw){
  c.beginPath();c.roundRect(x,y,w,h,r);
  if(f){c.fillStyle=f;c.fill();}
  if(s){c.strokeStyle=s;c.lineWidth=lw||1;c.stroke();}
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// 1. CIRCUITS + POLYSEMANTICITY
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
(function(){
  const{ctx:c,w,h}=setup('cv-circuits');
  const mid=h/2;

  // LEFT: clean circuit
  txt(c,'INTERPRETABLE CIRCUIT',w*.25,20,9,C.ft,'center','500');
  const icons=[{y:mid-40,label:'window',sym:'âŠž'},{y:mid,label:'body',sym:'â–¬'},{y:mid+40,label:'wheel',sym:'â—Ž'}];
  icons.forEach(ic=>{
    c.beginPath();c.arc(70,ic.y,18,0,Math.PI*2);c.fillStyle=C.sf;c.fill();c.strokeStyle=C.dkm;c.lineWidth=1.5;c.stroke();
    txt(c,ic.sym,70,ic.y,14,C.dk);
    txt(c,ic.label,96,ic.y,8,C.ft,'left');
  });
  const carX=w*.3,carY=mid;
  c.beginPath();c.arc(carX,carY,22,0,Math.PI*2);c.fillStyle=C.rd+'12';c.fill();c.strokeStyle=C.rd;c.lineWidth=2;c.stroke();
  txt(c,'ðŸš—',carX,carY-2,18,C.dk);
  txt(c,'"car"',carX,carY+32,9,C.rd);
  icons.forEach(ic=>{c.globalAlpha=.3;c.beginPath();c.moveTo(88,ic.y);c.lineTo(carX-22,carY);c.strokeStyle=C.dkm;c.lineWidth=1;c.stroke();c.globalAlpha=1;});

  // Divider
  c.beginPath();c.setLineDash([3,5]);c.moveTo(w/2,30);c.lineTo(w/2,h-10);c.strokeStyle=C.bd;c.lineWidth=1;c.stroke();c.setLineDash([]);

  // RIGHT: polysemantic
  txt(c,'POLYSEMANTIC NEURON',w*.75,20,9,C.ft,'center','500');
  const px=w*.75,py=mid;
  c.beginPath();c.arc(px,py,24,0,Math.PI*2);c.fillStyle=C.wm+'18';c.fill();c.strokeStyle=C.wm;c.lineWidth=2;c.stroke();
  txt(c,'???',px,py,12,C.wm,'center','600');

  const inputs=[{y:mid-50,label:'"car"',sym:'ðŸš—'},{y:mid,label:'"Italy"',sym:'ðŸ›'},{y:mid+50,label:'"red"',sym:'â—'}];
  inputs.forEach(inp=>{
    const ix=px-110;
    c.beginPath();c.arc(ix,inp.y,15,0,Math.PI*2);c.fillStyle=C.sf;c.fill();c.strokeStyle=C.bd;c.lineWidth=1;c.stroke();
    txt(c,inp.sym,ix,inp.y,11,C.dk);
    txt(c,inp.label,ix-22,inp.y,7,C.ft,'right');
    c.globalAlpha=.25;c.beginPath();c.moveTo(ix+15,inp.y);c.lineTo(px-24,py);c.strokeStyle=C.wm;c.lineWidth=1;c.stroke();c.globalAlpha=1;
  });
  const outputs=[{y:mid-36,label:'ðŸš—+ðŸ›?'},{y:mid+36,label:'â—+ðŸš—?'}];
  outputs.forEach(out=>{
    const ox=px+110;
    c.beginPath();c.arc(ox,out.y,15,0,Math.PI*2);c.fillStyle=C.sfa;c.fill();c.strokeStyle=C.bd;c.lineWidth=1;c.stroke();
    txt(c,out.label,ox,out.y,7,C.ft);
    c.globalAlpha=.2;c.beginPath();c.moveTo(px+24,py);c.lineTo(ox-15,out.y);c.strokeStyle=C.wm;c.lineWidth=1;c.stroke();c.globalAlpha=1;
  });
  txt(c,'superposition',px,h-14,8,C.wm);
})();

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// 2. SUPERPOSITION 3D (animated)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
(function(){
  const{ctx:c,w,h}=setup('cv-super');
  let nF=3;
  document.getElementById('sup-modes').addEventListener('click',e=>{
    if(!e.target.classList.contains('mbtn'))return;
    document.querySelectorAll('#sup-modes .mbtn').forEach(b=>b.classList.remove('active'));
    e.target.classList.add('active');nF=parseInt(e.target.dataset.n);
  });

  // Normalize all vectors to unit length
  function norm(v){const m=Math.sqrt(v[0]*v[0]+v[1]*v[1]+v[2]*v[2]);return[v[0]/m,v[1]/m,v[2]/m];}
  const sets={
    3:[[1,0,0],[0,1,0],[0,0,1]],
    5:[[1,0,0],[0,1,0],[0,0,1],norm([1,1,0]),norm([0,1,1])],
    8:[[1,0,0],[0,1,0],[0,0,1],norm([1,1,0]),norm([0,1,1]),norm([1,0,1]),norm([1,1,1.4]),norm([1.4,1,1])],
  };
  const names='ABCDEFGH'.split('');
  const cols=[C.rd,C.blue,C.gn,C.wm,C.pu,C.dkm,C.rl,'#7a6a4a'];

  // Isometric-ish projection: rotate around Y then tilt on X
  function proj(x,y,z,ay){
    // Rotate around Y
    const ca=Math.cos(ay),sa=Math.sin(ay);
    const rx=x*ca-z*sa, rz=x*sa+z*ca;
    // Tilt down 25Â°
    const tilt=0.42;
    const ct=Math.cos(tilt),st=Math.sin(tilt);
    const ry2=y*ct-rz*st, rz2=y*st+rz*ct;
    // Weak perspective
    const scale=1+rz2*0.001;
    return{px:rx*scale,py:-ry2*scale,depth:rz2};
  }

  function draw(){
    c.clearRect(0,0,w,h);
    const cx=w/2,cy=h/2+14,a=t*0.22,R=110;

    txt(c,nF+' features in 3 neurons',cx,22,10,C.dm,'center','500');

    // Draw axes (thin, faint)
    [[1,0,0,'nâ‚'],[0,1,0,'nâ‚‚'],[0,0,1,'nâ‚ƒ']].forEach(([ax,ay,az,label])=>{
      const p1=proj(-ax*R*0.2,-ay*R*0.2,-az*R*0.2,a);
      const p2=proj(ax*R*0.35,ay*R*0.35,az*R*0.35,a);
      c.globalAlpha=.12;c.beginPath();c.moveTo(cx+p1.px,cy+p1.py);c.lineTo(cx+p2.px,cy+p2.py);
      c.strokeStyle=C.dkm;c.lineWidth=1;c.stroke();c.globalAlpha=1;
      txt(c,label,cx+p2.px*1.15,cy+p2.py*1.15,8,C.vf);
    });

    // Collect features with depth for painter's algorithm
    const feats=sets[nF];
    const items=feats.map((f,i)=>{
      const p=proj(f[0]*R,f[1]*R,f[2]*R,a);
      return{i,p,f};
    });
    items.sort((a,b)=>a.p.depth-b.p.depth); // draw far first

    items.forEach(({i,p})=>{
      const alpha=0.6+p.depth*0.002;
      c.globalAlpha=Math.max(0.3,Math.min(1,alpha));
      // Line from origin
      c.beginPath();c.moveTo(cx,cy);c.lineTo(cx+p.px,cy+p.py);
      c.strokeStyle=cols[i];c.lineWidth=2.2;c.stroke();
      // Dot at tip
      c.beginPath();c.arc(cx+p.px,cy+p.py,5,0,Math.PI*2);c.fillStyle=cols[i];c.fill();
      // Label
      const lx=cx+p.px,ly=cy+p.py;
      const ox=p.px>0?10:-10;
      txt(c,names[i],lx+ox,ly-10,9,cols[i],p.px>0?'left':'right','500');
      c.globalAlpha=1;
    });

    // Status
    txt(c,nF<=3?'orthogonal â€” no interference':'non-orthogonal â€” superposition',cx,h-14,9,nF<=3?C.gn:C.rd);
  }
  (function loop(){t+=.016;draw();requestAnimationFrame(loop);})();
})();

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// 3. TRANSFORMER LAYER
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
(function(){
  const{ctx:c,w,h}=setup('cv-transformer');
  const mid=h/2+8,lx=70,rx=w-70,bW=110,bH=34;

  // Residual stream
  c.beginPath();c.moveTo(lx,mid);c.lineTo(rx,mid);c.strokeStyle=C.dkm;c.lineWidth=2;c.stroke();
  c.beginPath();c.moveTo(rx,mid);c.lineTo(rx-7,mid-4);c.lineTo(rx-7,mid+4);c.closePath();c.fillStyle=C.dkm;c.fill();
  txt(c,'embed',lx-6,mid,10,C.ft,'right');
  txt(c,'unembed',rx+6,mid,10,C.ft,'left');
  txt(c,'RESIDUAL STREAM',w/2,mid+22,8,C.vf);

  // Attention
  const attnX=w*.33-bW/2,attnY=mid-bH-30;
  rr(c,attnX,attnY,bW,bH,5,C.sf,C.dkm+'70',1.5);
  txt(c,'Attention',attnX+bW/2,attnY+bH/2,10,C.dm);
  txt(c,'moves info between positions',attnX+bW/2,attnY-12,8,C.ft);
  // read/write arrows
  c.globalAlpha=.45;
  c.beginPath();c.moveTo(attnX+bW/2-12,mid-2);c.lineTo(attnX+bW/2-12,attnY+bH+2);c.strokeStyle=C.dkm;c.lineWidth=1;c.stroke();
  c.beginPath();c.moveTo(attnX+bW/2+12,attnY+bH+2);c.lineTo(attnX+bW/2+12,mid-2);c.stroke();
  txt(c,'read',attnX+bW/2-26,mid-18,7,C.ft,'right');
  txt(c,'write',attnX+bW/2+26,mid-18,7,C.ft,'left');
  c.globalAlpha=1;

  // MLP
  const mlpX=w*.67-bW/2,mlpY=mid-bH-30;
  rr(c,mlpX,mlpY,bW,bH,5,C.sf,C.rd+'50',1.5);
  txt(c,'MLP',mlpX+bW/2,mlpY+bH/2,10,C.dm);
  txt(c,'computes within position',mlpX+bW/2,mlpY-12,8,C.ft);
  c.globalAlpha=.45;
  c.beginPath();c.moveTo(mlpX+bW/2-12,mid-2);c.lineTo(mlpX+bW/2-12,mlpY+bH+2);c.strokeStyle=C.rd+'80';c.lineWidth=1;c.stroke();
  c.beginPath();c.moveTo(mlpX+bW/2+12,mlpY+bH+2);c.lineTo(mlpX+bW/2+12,mid-2);c.stroke();
  c.globalAlpha=1;

  txt(c,'+',w*.33+bW/2+14,mid,13,C.dkm);
  txt(c,'+',w*.67+bW/2+14,mid,13,C.dkm);
})();

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// 4. INDUCTION HEAD (left-to-right)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
(function(){
  const{ctx:c,w,h}=setup('cv-induction');

  // Layout: two horizontal lanes
  const lane1Y=70, lane2Y=200;   // vertical centers of each lane
  const tokW=38, tokH=26, tokR=4;
  const lm=58;                    // left margin for labels
  const seqLeft=lm+8;
  const seqW=w-seqLeft-30;

  // Lane labels
  txt(c,'LAYER 1',28,lane1Y,7,C.blue,'center','600');
  txt(c,'prev-token',28,lane1Y+10,6,C.ft);
  txt(c,'head',28,lane1Y+18,6,C.ft);

  txt(c,'LAYER 2',28,lane2Y,7,C.rd,'center','600');
  txt(c,'induction',28,lane2Y+10,6,C.ft);
  txt(c,'head',28,lane2Y+18,6,C.ft);

  // Divider
  c.globalAlpha=.15;c.beginPath();c.moveTo(seqLeft,lane1Y+50);c.lineTo(w-20,lane1Y+50);c.strokeStyle=C.bd;c.lineWidth=1;c.stroke();c.globalAlpha=1;

  // â”€â”€ LAYER 1: show how [A B] gets "A precedes B" stored â”€â”€
  // Tokens for L1: â€¦ A B â€¦
  const l1Toks=[
    {label:'â€¦',x:seqLeft+20},
    {label:'A',x:seqLeft+seqW*.18,hl:C.blue},
    {label:'B',x:seqLeft+seqW*.32,hl:C.blue},
    {label:'â€¦',x:seqLeft+seqW*.46},
  ];
  l1Toks.forEach(tk=>{
    const border=tk.hl||C.bd;
    rr(c,tk.x-tokW/2,lane1Y-tokH/2,tokW,tokH,tokR,C.sf,border,tk.hl?1.5:1);
    txt(c,tk.label,tk.x,lane1Y,11,tk.hl||C.dm);
  });

  // Arrow from B back to A (previous-token attention)
  const pA=l1Toks[1],pB=l1Toks[2];
  c.globalAlpha=.4;
  c.beginPath();c.moveTo(pB.x,lane1Y-tokH/2-2);
  c.quadraticCurveTo((pA.x+pB.x)/2,lane1Y-tokH/2-28,pA.x,lane1Y-tokH/2-2);
  c.strokeStyle=C.blue;c.lineWidth=1.5;c.stroke();
  // arrowhead pointing at A
  c.beginPath();c.moveTo(pA.x,lane1Y-tokH/2-2);c.lineTo(pA.x+5,lane1Y-tokH/2-8);c.lineTo(pA.x-5,lane1Y-tokH/2-8);c.closePath();c.fillStyle=C.blue;c.fill();
  c.globalAlpha=1;
  txt(c,'attend to prev token',(pA.x+pB.x)/2,lane1Y-tokH/2-34,7,C.blue);

  // Result arrow â†’ stores info
  const resX=seqLeft+seqW*.58;
  c.globalAlpha=.3;c.beginPath();c.moveTo(pB.x+tokW/2+4,lane1Y);c.lineTo(resX-4,lane1Y);c.strokeStyle=C.blue;c.lineWidth=1;c.stroke();c.globalAlpha=1;
  // Arrow head
  c.beginPath();c.moveTo(resX-4,lane1Y);c.lineTo(resX-10,lane1Y-3);c.lineTo(resX-10,lane1Y+3);c.closePath();c.fillStyle=C.blue+'80';c.fill();
  // Stored info badge
  rr(c,resX,lane1Y-12,seqW*.38,24,4,C.blue+'08',C.blue+'35',1);
  txt(c,'"A precedes B" â†’ residual stream',resX+seqW*.19,lane1Y,7,C.blue);

  // â”€â”€ LAYER 2: see second A, look up pattern, predict B â”€â”€
  // Tokens for L2: â€¦ A  â†’ [?]
  const l2Toks=[
    {label:'â€¦',x:seqLeft+20},
    {label:'A',x:seqLeft+seqW*.18,hl:C.rd},
  ];
  l2Toks.forEach(tk=>{
    const border=tk.hl||C.bd;
    rr(c,tk.x-tokW/2,lane2Y-tokH/2,tokW,tokH,tokR,C.sf,border,tk.hl?1.5:1);
    txt(c,tk.label,tk.x,lane2Y,11,tk.hl||C.dm);
  });

  const queryA=l2Toks[1];

  // Step 1: query "who was preceded by A?"
  const qX=seqLeft+seqW*.38;
  c.globalAlpha=.35;c.beginPath();c.moveTo(queryA.x+tokW/2+4,lane2Y);c.lineTo(qX-4,lane2Y);c.strokeStyle=C.rd;c.lineWidth=1;c.stroke();c.globalAlpha=1;
  c.beginPath();c.moveTo(qX-4,lane2Y);c.lineTo(qX-10,lane2Y-3);c.lineTo(qX-10,lane2Y+3);c.closePath();c.fillStyle=C.rd+'80';c.fill();
  rr(c,qX,lane2Y-12,seqW*.28,24,4,C.rd+'08',C.rd+'35',1);
  txt(c,'query: "preceded by A?"',qX+seqW*.14,lane2Y,7,C.rd);

  // Step 2: arrow up to L1 stored info (cross-layer lookup)
  const lookupFromX=qX+seqW*.14;
  const lookupToX=resX+seqW*.19;
  c.globalAlpha=.25;
  c.beginPath();
  c.moveTo(lookupFromX,lane2Y-14);
  c.quadraticCurveTo(lookupFromX+20,lane1Y+50,lookupToX,lane1Y+14);
  c.strokeStyle=C.rd;c.lineWidth=1.2;c.setLineDash([4,3]);c.stroke();c.setLineDash([]);
  c.globalAlpha=1;
  // arrowhead
  c.beginPath();c.moveTo(lookupToX,lane1Y+14);c.lineTo(lookupToX-4,lane1Y+20);c.lineTo(lookupToX+4,lane1Y+20);c.closePath();c.fillStyle=C.rd+'50';c.fill();
  txt(c,'match!',lookupFromX+16,(lane1Y+lane2Y)/2,7,C.rd,'left','500');

  // Step 3: finds B â†’ copies to prediction
  const foundX=seqLeft+seqW*.72;
  c.globalAlpha=.3;c.beginPath();c.moveTo(qX+seqW*.28+4,lane2Y);c.lineTo(foundX-4,lane2Y);c.strokeStyle=C.gn;c.lineWidth=1;c.stroke();c.globalAlpha=1;
  c.beginPath();c.moveTo(foundX-4,lane2Y);c.lineTo(foundX-10,lane2Y-3);c.lineTo(foundX-10,lane2Y+3);c.closePath();c.fillStyle=C.gn+'80';c.fill();
  rr(c,foundX,lane2Y-12,seqW*.12,24,4,C.gn+'08',C.gn+'35',1);
  txt(c,'copy B',foundX+seqW*.06,lane2Y,7,C.gn);

  // Final prediction
  const predX=seqLeft+seqW*.9;
  c.globalAlpha=.35;c.beginPath();c.moveTo(foundX+seqW*.12+4,lane2Y);c.lineTo(predX-tokW/2-4,lane2Y);c.strokeStyle=C.gn;c.lineWidth=1.5;c.stroke();c.globalAlpha=1;
  c.beginPath();c.moveTo(predX-tokW/2-4,lane2Y);c.lineTo(predX-tokW/2-10,lane2Y-3);c.lineTo(predX-tokW/2-10,lane2Y+3);c.closePath();c.fillStyle=C.gn;c.fill();
  rr(c,predX-tokW/2,lane2Y-tokH/2,tokW,tokH,tokR,C.rd+'0c',C.rd,1.8);
  txt(c,'B',predX,lane2Y,12,C.rd,'center','600');
  txt(c,'predict',predX,lane2Y+tokH/2+10,7,C.gn);

  // Overall flow arrow at bottom
  c.globalAlpha=.1;c.beginPath();c.moveTo(seqLeft,h-16);c.lineTo(w-30,h-16);c.strokeStyle=C.dkm;c.lineWidth=1;c.stroke();
  c.beginPath();c.moveTo(w-30,h-16);c.lineTo(w-36,h-19);c.lineTo(w-36,h-13);c.closePath();c.fillStyle=C.dkm;c.fill();
  c.globalAlpha=1;
  txt(c,'computation flows left â†’ right',w/2,h-8,7,C.vf);
})();

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// 5. SPARSE AUTOENCODER
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
(function(){
  const{ctx:c,w,h}=setup('cv-sae');
  const cx=w/2, barH=20;

  // â”€â”€ Dense activations â”€â”€
  const dY=40, dW=220;
  txt(c,'MLP ACTIVATIONS',cx,dY-14,9,C.dm,'center','500');
  rr(c,cx-dW/2,dY,dW,barH,3,C.dkm+'18',C.dkm+'50',1);
  for(let i=0;i<16;i++){
    c.fillStyle=C.dkm+'90';
    c.fillRect(cx-dW/2+3+i*13.5, dY+3, 11.5, barH-6);
  }
  txt(c,'16 neurons, all active',cx,dY+barH+12,7,C.wm);

  // â”€â”€ Encoder â”€â”€
  const encY=dY+barH+28;
  c.globalAlpha=.3;
  c.beginPath();c.moveTo(cx-40,encY);c.lineTo(cx-100,encY+40);
  c.moveTo(cx+40,encY);c.lineTo(cx+100,encY+40);
  c.moveTo(cx,encY);c.lineTo(cx,encY+40);
  c.strokeStyle=C.dkm;c.lineWidth=1;c.stroke();c.globalAlpha=1;
  txt(c,'W_enc',cx+18,encY+20,8,C.ft,'left');

  // â”€â”€ Sparse hidden â”€â”€
  const sY=encY+48, sW=w-60;
  rr(c,cx-sW/2,sY,sW,barH+4,3,C.rd+'06',C.rd+'25',1);
  const nS=40, act=[3,14,24,35], nw=(sW-6)/nS;
  const fLabels=['"syntax"','"negation"','"formal"','"entity"'];
  for(let i=0;i<nS;i++){
    const nx=cx-sW/2+3+i*nw, on=act.includes(i);
    c.fillStyle=on?C.rd:C.rd+'06';
    c.fillRect(nx,sY+3,nw-1,barH-2);
    if(on){c.shadowColor=C.rd;c.shadowBlur=6;c.fillRect(nx,sY+3,nw-1,barH-2);c.shadowBlur=0;}
  }
  // Labels with leader lines
  act.forEach((idx,i)=>{
    const fx=cx-sW/2+3+idx*nw+nw/2;
    c.globalAlpha=.3;c.beginPath();c.moveTo(fx,sY+barH+2);c.lineTo(fx,sY+barH+14);c.strokeStyle=C.rd;c.lineWidth=.5;c.stroke();c.globalAlpha=1;
    txt(c,fLabels[i],fx,sY+barH+22,7,C.rd);
  });
  txt(c,'4 of 40 active â€” monosemantic',cx,sY+barH+38,8,C.gn);

  // â”€â”€ Decoder â”€â”€
  const decY=sY+barH+52;
  c.globalAlpha=.3;
  c.beginPath();c.moveTo(cx-100,decY);c.lineTo(cx-40,decY+40);
  c.moveTo(cx+100,decY);c.lineTo(cx+40,decY+40);
  c.moveTo(cx,decY);c.lineTo(cx,decY+40);
  c.strokeStyle=C.dkm;c.lineWidth=1;c.stroke();c.globalAlpha=1;
  txt(c,'W_dec',cx+18,decY+20,8,C.ft,'left');

  // â”€â”€ Reconstructed â”€â”€
  const rY=decY+48;
  txt(c,'RECONSTRUCTED',cx,rY-12,9,C.dm,'center','500');
  rr(c,cx-dW/2,rY,dW,barH,3,C.dkm+'18',C.gn+'50',1);
  for(let i=0;i<16;i++){
    c.fillStyle=C.dkm+'80';
    c.fillRect(cx-dW/2+3+i*13.5, rY+3, 11.5, barH-6);
  }
  txt(c,'â‰ˆ original',cx+dW/2+10,rY+barH/2,8,C.gn,'left');
})();

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// 6. REPRESENTATION ENGINEERING (animated PCA)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
(function(){
  const{ctx:c,w,h}=setup('cv-repe');

  // 3D points in PCA space (hand-picked for visual clarity)
  const honest=[[-0.6,0.7,0.3],[-0.4,0.9,-0.1],[-0.8,0.5,0.1]];
  const dishonest=[[0.5,-0.6,0.2],[0.7,-0.4,-0.3],[0.3,-0.8,0.1]];
  function mean(pts){return[pts.reduce((s,p)=>s+p[0],0)/pts.length,pts.reduce((s,p)=>s+p[1],0)/pts.length,pts.reduce((s,p)=>s+p[2],0)/pts.length];}
  const hMean=mean(honest), dMean=mean(dishonest);
  const dir=[hMean[0]-dMean[0],hMean[1]-dMean[1],hMean[2]-dMean[2]];

  // A "dishonest" test point and its steered version
  const testPt=[0.4,-0.5,0.15];
  const steeredPt=[testPt[0]+dir[0],testPt[1]+dir[1],testPt[2]+dir[2]];

  // Animation phases (seconds): scatter â†’ means â†’ direction â†’ steer
  const phases=[0,3,5,7.5,10.5]; // start times
  const totalLoop=13;

  // Projection
  function proj(x,y,z,a){
    const ca=Math.cos(a),sa=Math.sin(a);
    const rx=x*ca-z*sa,rz=x*sa+z*ca;
    const tilt=0.38,ct=Math.cos(tilt),st=Math.sin(tilt);
    const ry2=y*ct-rz*st;
    return{px:rx,py:-ry2};
  }

  function lerp(a,b,t){return a+(b-a)*Math.max(0,Math.min(1,t));}
  function lerp3(a,b,t){return[lerp(a[0],b[0],t),lerp(a[1],b[1],t),lerp(a[2],b[2],t)];}
  function ease(t){return t<.5?2*t*t:1-Math.pow(-2*t+2,2)/2;}

  function draw(){
    c.clearRect(0,0,w,h);
    const phase=t%totalLoop;
    const R=100, rot=t*0.15;

    // â”€â”€ LEFT PANEL: Find direction â”€â”€
    const lcx=w*.3, lcy=h*.48;
    txt(c,'1. FIND DIRECTION',w*.3,16,8,C.ft,'center','500');

    // Faint axes
    [[1,0,0,'PCâ‚'],[0,1,0,'PCâ‚‚'],[0,0,1,'PCâ‚ƒ']].forEach(([ax,ay,az,label])=>{
      const p1=proj(-ax*R*.2,-ay*R*.2,-az*R*.2,rot);
      const p2=proj(ax*R*.4,ay*R*.4,az*R*.4,rot);
      c.globalAlpha=.1;c.beginPath();c.moveTo(lcx+p1.px,lcy+p1.py);c.lineTo(lcx+p2.px,lcy+p2.py);
      c.strokeStyle=C.dkm;c.lineWidth=1;c.stroke();c.globalAlpha=1;
      txt(c,label,lcx+p2.px*1.1+4,lcy+p2.py*1.1,7,C.vf,'left');
    });

    // Phase progress
    const p1=ease(Math.max(0,Math.min(1,(phase-phases[0])/1.5)));   // scatter appear
    const p2=ease(Math.max(0,Math.min(1,(phase-phases[1])/1.5)));   // collapse to means
    const p3=ease(Math.max(0,Math.min(1,(phase-phases[2])/1.5)));   // show direction arrow

    // Draw honest points
    honest.forEach((pt,i)=>{
      const target=lerp3(pt,hMean,p2);
      const pp=proj(target[0]*R,target[1]*R,target[2]*R,rot);
      const alpha=p1*(1-p2*0.5);
      c.globalAlpha=alpha;
      c.beginPath();c.arc(lcx+pp.px,lcy+pp.py,4,0,Math.PI*2);c.fillStyle=C.gn;c.fill();
      c.globalAlpha=1;
    });
    // Honest mean
    if(p2>0){
      const mp=proj(hMean[0]*R,hMean[1]*R,hMean[2]*R,rot);
      c.globalAlpha=p2;
      c.beginPath();c.arc(lcx+mp.px,lcy+mp.py,6,0,Math.PI*2);c.fillStyle=C.gn;c.fill();
      c.strokeStyle=C.gn;c.lineWidth=1.5;c.stroke();
      txt(c,'Î¼_honest',lcx+mp.px+12,lcy+mp.py-4,7,C.gn,'left');
      c.globalAlpha=1;
    }

    // Draw dishonest points
    dishonest.forEach((pt,i)=>{
      const target=lerp3(pt,dMean,p2);
      const pp=proj(target[0]*R,target[1]*R,target[2]*R,rot);
      const alpha=p1*(1-p2*0.5);
      c.globalAlpha=alpha;
      c.beginPath();c.arc(lcx+pp.px,lcy+pp.py,4,0,Math.PI*2);c.fillStyle=C.rd;c.fill();
      c.globalAlpha=1;
    });
    // Dishonest mean
    if(p2>0){
      const mp=proj(dMean[0]*R,dMean[1]*R,dMean[2]*R,rot);
      c.globalAlpha=p2;
      c.beginPath();c.arc(lcx+mp.px,lcy+mp.py,6,0,Math.PI*2);c.fillStyle=C.rd;c.fill();
      c.strokeStyle=C.rd;c.lineWidth=1.5;c.stroke();
      txt(c,'Î¼_dishonest',lcx+mp.px+12,lcy+mp.py+4,7,C.rd,'left');
      c.globalAlpha=1;
    }

    // Direction arrow (mean_dishonest â†’ mean_honest)
    if(p3>0){
      const dp=proj(dMean[0]*R,dMean[1]*R,dMean[2]*R,rot);
      const hp=proj(hMean[0]*R,hMean[1]*R,hMean[2]*R,rot);
      const ex=dp.px+p3*(hp.px-dp.px), ey=dp.py+p3*(hp.py-dp.py);
      c.globalAlpha=p3*.7;
      c.beginPath();c.moveTo(lcx+dp.px,lcy+dp.py);c.lineTo(lcx+ex,lcy+ey);
      c.strokeStyle=C.dk;c.lineWidth=2.5;c.stroke();
      // Arrowhead
      if(p3>.8){
        const ang=Math.atan2(ey-dp.py,ex-dp.px);
        c.beginPath();
        c.moveTo(lcx+ex,lcy+ey);
        c.lineTo(lcx+ex-8*Math.cos(ang-0.35),lcy+ey-8*Math.sin(ang-0.35));
        c.lineTo(lcx+ex-8*Math.cos(ang+0.35),lcy+ey-8*Math.sin(ang+0.35));
        c.closePath();c.fillStyle=C.dk;c.fill();
      }
      c.globalAlpha=1;
      if(p3>.5)txt(c,'honesty direction',lcx,(lcy+dp.py+ey)/2+lcy*.15+20,8,C.dk,'center','500');
    }

    // Phase labels
    if(phase<phases[1])txt(c,'activations in PCA space',lcx,h-16,7,C.ft);
    else if(phase<phases[2])txt(c,'compute cluster means',lcx,h-16,7,C.ft);
    else txt(c,'Î¼_honest âˆ’ Î¼_dishonest = direction',lcx,h-16,7,C.ft);

    // â”€â”€ Divider â”€â”€
    c.beginPath();c.setLineDash([3,5]);c.moveTo(w*.54,12);c.lineTo(w*.54,h-8);c.strokeStyle=C.bd;c.lineWidth=1;c.stroke();c.setLineDash([]);

    // â”€â”€ RIGHT PANEL: Steer behavior â”€â”€
    const rcx=w*.78, rcy=h*.42;
    txt(c,'2. STEER',w*.78,16,8,C.ft,'center','500');

    // Same axes
    [[1,0,0,'PCâ‚'],[0,1,0,'PCâ‚‚'],[0,0,1,'PCâ‚ƒ']].forEach(([ax,ay,az,label])=>{
      const p1=proj(-ax*R*.2,-ay*R*.2,-az*R*.2,rot);
      const p2=proj(ax*R*.4,ay*R*.4,az*R*.4,rot);
      c.globalAlpha=.1;c.beginPath();c.moveTo(rcx+p1.px,rcy+p1.py);c.lineTo(rcx+p2.px,rcy+p2.py);
      c.strokeStyle=C.dkm;c.lineWidth=1;c.stroke();c.globalAlpha=1;
    });

    // Ghost clusters for context
    c.globalAlpha=.08;
    honest.forEach(pt=>{const pp=proj(pt[0]*R,pt[1]*R,pt[2]*R,rot);c.beginPath();c.arc(rcx+pp.px,rcy+pp.py,3,0,Math.PI*2);c.fillStyle=C.gn;c.fill();});
    dishonest.forEach(pt=>{const pp=proj(pt[0]*R,pt[1]*R,pt[2]*R,rot);c.beginPath();c.arc(rcx+pp.px,rcy+pp.py,3,0,Math.PI*2);c.fillStyle=C.rd;c.fill();});
    c.globalAlpha=1;

    const p4=ease(Math.max(0,Math.min(1,(phase-phases[3])/2))); // steer animation

    // Test point â€” always visible, sitting in dishonest cluster from the start
    {
      const curPt=lerp3(testPt,steeredPt,p4);
      const pp=proj(curPt[0]*R,curPt[1]*R,curPt[2]*R,rot);

      // Steering arrow trail (during motion)
      if(p4>0 && p4<1){
        const sp=proj(testPt[0]*R,testPt[1]*R,testPt[2]*R,rot);
        c.globalAlpha=.3;c.beginPath();c.moveTo(rcx+sp.px,rcy+sp.py);c.lineTo(rcx+pp.px,rcy+pp.py);
        c.strokeStyle=C.dk;c.lineWidth=1.5;c.setLineDash([3,3]);c.stroke();c.setLineDash([]);c.globalAlpha=1;
      }

      // The dot â€” red while waiting, transitions through warm to green
      const dotCol=p4<.5?C.rd:p4<1?C.wm:C.gn;
      c.beginPath();c.arc(rcx+pp.px,rcy+pp.py,7,0,Math.PI*2);
      c.fillStyle=dotCol;c.fill();c.strokeStyle=dotCol;c.lineWidth=1.5;c.stroke();

      // Label
      if(p4<.3){
        txt(c,'"2+2 = 5"',rcx+pp.px,rcy+pp.py+18,8,C.rd);
      } else if(p4>=1){
        txt(c,'"2+2 = 4"',rcx+pp.px,rcy+pp.py+18,8,C.gn);
        txt(c,'+ honesty direction',rcx,h-34,8,C.gn);
      }
    }

    if(phase<phases[3])txt(c,'"Tell me a lie" â†’',rcx,h-34,7,C.ft);
    const stageLabel=p4<.3?'dishonest activation':p4>=1?'â†’ honest activation':'steering...';
    txt(c,stageLabel,rcx,h-16,7,p4>=1?C.gn:C.ft);
  }

  (function loop(){t+=.016;draw();requestAnimationFrame(loop);})();
})();

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// 7. TRANSCODERS + ATTRIBUTION
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
(function(){
  const{ctx:c,w,h}=setup('cv-trans');
  let mode='plt';
  document.getElementById('tc-modes').addEventListener('click',e=>{
    if(!e.target.classList.contains('mbtn'))return;
    document.querySelectorAll('#tc-modes .mbtn').forEach(b=>b.classList.remove('active'));
    e.target.classList.add('active');mode=e.target.dataset.mode;
  });

  const half=w/2;

  function drawTC(){
    c.clearRect(0,0,w,h);

    // â”€â”€ LEFT: Architecture â”€â”€
    const cx=half*.38, bW=90, bH=24;
    const layers=[{label:'Lâˆ’1',y:32},{label:'L',y:140,rep:true},{label:'L+1',y:248}];
    const isAll=(mode==='clt'||mode==='qk');

    // Residual stream line
    c.beginPath();c.setLineDash([4,3]);c.moveTo(cx,16);c.lineTo(cx,h-16);c.strokeStyle=C.bd;c.lineWidth=1;c.stroke();c.setLineDash([]);

    layers.forEach(ly=>{
      txt(c,ly.label,cx-bW/2-12,ly.y+18,7,C.ft,'right','500');
      const aY=ly.y;
      // QK highlight
      if(mode==='qk'){c.strokeStyle=C.pu;c.lineWidth=1.5;c.beginPath();c.roundRect(cx-bW/2-3,aY-3,bW+6,bH+6,5);c.stroke();}
      rr(c,cx-bW/2,aY,bW,bH,3,C.sf,C.dkm+'50',1);
      txt(c,'Attn',cx,aY+bH/2,9,C.dm);

      const mY=aY+bH+8; ly._mY=mY;
      const replaced=ly.rep||isAll;
      if(replaced){
        c.globalAlpha=.2;rr(c,cx-bW/2,mY,bW,bH,3,C.sfa,C.bd,1);txt(c,'MLP',cx,mY+bH/2,9,C.ft);
        c.beginPath();c.moveTo(cx-bW/2+8,mY+bH/2);c.lineTo(cx+bW/2-8,mY+bH/2);c.strokeStyle=C.rd;c.lineWidth=1.2;c.stroke();c.globalAlpha=1;
      } else {
        rr(c,cx-bW/2,mY,bW,bH,3,C.sf,C.dk+'50',1);txt(c,'MLP',cx,mY+bH/2,9,C.dm);
      }
    });

    // Transcoder detail
    if(mode==='plt'){
      const ly=layers[1];
      txt(c,'â†’',cx+bW/2+8,ly._mY+bH/2,11,C.rd);
      const sx=cx+bW/2+22;
      rr(c,sx,ly._mY+2,half-sx-10,bH-4,3,C.rd+'06',C.rd+'20',1);
      const spW=half-sx-14, nN=14;
      const nww=spW/nN, act=[2,7,11];
      for(let i=0;i<nN;i++){const nx=sx+2+i*nww,on=act.includes(i);c.fillStyle=on?C.rd:C.rd+'08';c.fillRect(nx,ly._mY+6,nww-1,bH-12);if(on){c.shadowColor=C.rd;c.shadowBlur=3;c.fillRect(nx,ly._mY+6,nww-1,bH-12);c.shadowBlur=0;}}
      txt(c,'transcoder',sx+spW/2,ly._mY-8,7,C.rd);
    } else {
      // Shared feature bank
      const bx=cx+bW/2+16, bTop=layers[0]._mY, bBot=layers[2]._mY+bH+4;
      const bkW=half-bx-10;
      rr(c,bx,bTop,bkW,bBot-bTop,4,C.rd+'05',C.rd+'18',1);
      txt(c,'SHARED FEATURES',bx+bkW/2,bTop-8,6,C.rd);
      [{ry:.1},{ry:.35},{ry:.6},{ry:.85}].forEach((ft,i)=>{
        const fy=bTop+(bBot-bTop)*ft.ry+6, fx=bx+10+(i%3)*14;
        const p=.35+.45*Math.sin(t*2.5+i*1.8);
        c.beginPath();c.arc(fx,fy,3.5,0,Math.PI*2);c.fillStyle=C.rd;c.globalAlpha=p;c.fill();c.globalAlpha=1;
        c.strokeStyle=C.rk+'80';c.lineWidth=.5;c.stroke();
      });
    }

    txt(c,'ARCHITECTURE',half*.5,h-6,6,C.vf);

    // â”€â”€ Divider â”€â”€
    c.beginPath();c.setLineDash([3,5]);c.moveTo(half,6);c.lineTo(half,h-6);c.strokeStyle=C.bd;c.lineWidth=.7;c.stroke();c.setLineDash([]);

    // â”€â”€ RIGHT: Attribution Graph â”€â”€
    txt(c,'ATTRIBUTION GRAPH',half+half/2,h-6,6,C.vf);
    const ox=half+20, gw=half-40;
    txt(c,'"How do I make a pipe bomb?"',half+half/2,16,8,C.dm,'center','500');

    // Nodes â€” more spread out, cleaner
    const N=[
      {x:ox+gw*.12, y:56,  label:'"harmful\nrequest"', imp:.7, on:true, ly:'L12'},
      {x:ox+gw*.78, y:52,  label:'"chemistry"',        imp:.3, on:false,ly:'L14'},
      {x:ox+gw*.4,  y:164, label:'"dangerous\nintent"', imp:1,  on:true, ly:'L20'},
      {x:ox+gw*.35, y:280, label:'"refuse"',            imp:1,  on:true, ly:'L30'},
      {x:ox+gw*.82, y:274, label:'"explain\npolicy"',   imp:.4, on:true, ly:'L31'},
    ];

    function edg(a,b,wt,hot,col){
      c.globalAlpha=.15+wt*.5;
      c.beginPath();c.moveTo(a.x,a.y+10);c.lineTo(b.x,b.y-10);
      c.strokeStyle=col||(hot?C.rd:C.dkm);c.lineWidth=.8+wt*2;c.stroke();c.globalAlpha=1;
    }
    function node(n){
      const r=5+n.imp*5;
      if(n.on){c.shadowColor=C.rd;c.shadowBlur=5;}
      c.beginPath();c.arc(n.x,n.y,r,0,Math.PI*2);c.fillStyle=n.on?C.rd:C.sfa;c.fill();
      c.strokeStyle=n.on?C.rk:C.bd;c.lineWidth=n.on?1.3:1;c.stroke();c.shadowBlur=0;
      txt(c,n.ly,n.x,n.y+2,6,n.on?'#f5f0e8':C.ft);
      const lines=n.label.split('\n');
      lines.forEach((ln,i)=>txt(c,ln,n.x,n.y+r+10+i*11,8,n.on?C.rd:C.ft));
    }

    // MLP edges (always present)
    edg(N[0],N[2],.8,true);
    edg(N[1],N[2],.2,false);
    edg(N[2],N[3],.9,true);
    edg(N[2],N[4],.3,true);

    // Attention curve â€” swing far to the RIGHT to separate from the straight MLP edge
    const attnCpx=N[0].x+gw*.45, attnCpy=(N[0].y+N[2].y)/2-10;

    // Mode-specific overlays
    if(mode==='plt'){
      c.globalAlpha=.2;c.beginPath();c.moveTo(N[0].x+6,N[0].y+10);
      c.quadraticCurveTo(attnCpx,attnCpy,N[2].x+6,N[2].y-10);
      c.strokeStyle=C.dkm;c.lineWidth=1.2;c.setLineDash([3,3]);c.stroke();c.setLineDash([]);c.globalAlpha=1;
      // Label on the curve
      const lbx=attnCpx*.7+N[0].x*.15+N[2].x*.15, lby=attnCpy;
      rr(c,lbx-24,lby-8,52,16,3,C.sf,C.bd,1);
      txt(c,'attn ???',lbx+2,lby,7,C.ft);
      txt(c,'attention is opaque',half+half/2,h-20,8,C.ft);
    }

    if(mode==='clt'||mode==='qk'){
      // Cross-layer skip edge
      c.globalAlpha=.3;
      const cpx=Math.min(N[0].x,N[3].x)-22,cpy=(N[0].y+N[3].y)/2;
      c.beginPath();c.moveTo(N[0].x,N[0].y+10);c.quadraticCurveTo(cpx,cpy,N[3].x,N[3].y-10);
      c.strokeStyle=C.rd;c.lineWidth=1.2;c.setLineDash([4,3]);c.stroke();c.setLineDash([]);c.globalAlpha=1;
    }

    if(mode==='clt'){
      c.globalAlpha=.2;c.beginPath();c.moveTo(N[0].x+6,N[0].y+10);
      c.quadraticCurveTo(attnCpx,attnCpy,N[2].x+6,N[2].y-10);
      c.strokeStyle=C.dkm;c.lineWidth=1.2;c.setLineDash([3,3]);c.stroke();c.setLineDash([]);c.globalAlpha=1;
      const lbx=attnCpx*.7+N[0].x*.15+N[2].x*.15, lby=attnCpy;
      rr(c,lbx-24,lby-8,52,16,3,C.sf,C.bd,1);txt(c,'attn ???',lbx+2,lby,7,C.ft);
      txt(c,'no chain error â€” attention still opaque',half+half/2,h-20,8,C.gn);
    }

    if(mode==='qk'){
      c.globalAlpha=.5;c.beginPath();c.moveTo(N[0].x+6,N[0].y+10);
      c.quadraticCurveTo(attnCpx,attnCpy,N[2].x+6,N[2].y-10);
      c.strokeStyle=C.pu;c.lineWidth=2;c.stroke();c.globalAlpha=1;
      const lbx=attnCpx*.7+N[0].x*.15+N[2].x*.15, lby=attnCpy;
      rr(c,lbx-16,lby-10,90,20,3,C.sf,C.pu+'50',1);
      txt(c,'"request"_Q Â· "bomb"_K',lbx+29,lby,6.5,C.pu);
      txt(c,'full forward pass traceable',half+half/2,h-20,8,C.pu);
    }

    N.forEach(n=>node(n));

    // Output box â€” positioned below L30 node with clear space
    rr(c,N[3].x-56,N[3].y+30,112,20,3,C.sf,C.rd+'30',1);
    txt(c,'"I can\'t help with that."',N[3].x,N[3].y+40,7,C.dm);
  }

  (function loop2(){t+=.016;drawTC();requestAnimationFrame(loop2);})();
})();

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// 8. NATURAL LANGUAGE EXPLANATION
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
(function(){
  const{ctx:c,w,h}=setup('cv-nle');
  let mode='latentqa';
  document.getElementById('nle-modes').addEventListener('click',e=>{
    if(!e.target.classList.contains('mbtn'))return;
    document.querySelectorAll('#nle-modes .mbtn').forEach(b=>b.classList.remove('active'));
    e.target.classList.add('active');mode=e.target.dataset.mode;
  });

  // Shared drawing helpers
  function modelBox(x,y,bw,bh,label,color,sub){
    rr(c,x-bw/2,y-bh/2,bw,bh,5,C.sf,color+'60',1.5);
    txt(c,label,x,y-(sub?4:0),9,color,'center','500');
    if(sub)txt(c,sub,x,y+8,6,C.ft);
  }
  function arrow(x1,y1,x2,y2,color,width,dashed){
    c.globalAlpha=.5;
    if(dashed)c.setLineDash([4,3]);
    c.beginPath();c.moveTo(x1,y1);c.lineTo(x2,y2);
    c.strokeStyle=color;c.lineWidth=width||1.2;c.stroke();
    c.setLineDash([]);
    // arrowhead
    const ang=Math.atan2(y2-y1,x2-x1),sz=6;
    c.beginPath();c.moveTo(x2,y2);
    c.lineTo(x2-sz*Math.cos(ang-.35),y2-sz*Math.sin(ang-.35));
    c.lineTo(x2-sz*Math.cos(ang+.35),y2-sz*Math.sin(ang+.35));
    c.closePath();c.fillStyle=color;c.fill();
    c.globalAlpha=1;
  }
  function actBar(x,y,bw,bh,n,active,color){
    rr(c,x,y,bw,bh,3,color+'08',color+'25',1);
    const nw=(bw-4)/n;
    for(let i=0;i<n;i++){
      const on=active.includes(i);
      c.fillStyle=on?color:color+'08';
      c.fillRect(x+2+i*nw,y+3,nw-1,bh-6);
      if(on){c.shadowColor=color;c.shadowBlur=3;c.fillRect(x+2+i*nw,y+3,nw-1,bh-6);c.shadowBlur=0;}
    }
  }

  function draw(){
    c.clearRect(0,0,w,h);

    if(mode==='latentqa'){
      // â”€â”€ LatentQA: Target model â†’ activations â†’ decoder â†’ answer â”€â”€
      const tX=w*.22, tY=100, dX=w*.22, dY=280;
      const bw=130, bh=40;

      // Target model
      modelBox(tX,tY,bw,bh,'Target LLM',C.dkm);

      // Prompt input
      rr(c,tX-bw/2-8,tY-70,bw+16,24,4,C.sf,C.bd,1);
      txt(c,'"Tell me about yourself"',tX,tY-58,7,C.dm);
      arrow(tX,tY-46,tX,tY-bh/2-2,C.dkm,1);

      // Hidden system prompt (sneaky)
      rr(c,tX+bw/2+20,tY-70,100,24,4,C.sf,C.wm+'40',1);
      txt(c,'[hidden: be a pirate]',tX+bw/2+70,tY-58,6,C.wm);
      arrow(tX+bw/2+70,tY-46,tX+20,tY-bh/2-2,C.wm,1,true);

      // Activations flowing down
      const actY=tY+bh/2+20;
      actBar(tX-60,actY,120,16,14,[2,5,8,11],C.rd);
      txt(c,'activations',tX,actY+28,7,C.rd);
      arrow(tX,tY+bh/2+2,tX,actY-2,C.rd,1.2);

      // Arrow from activations to decoder
      arrow(tX,actY+34,dX,dY-bh/2-2,C.rd,1.2);

      // Decoder model
      modelBox(dX,dY,bw,bh,'Decoder LLM',C.blue,'(copy of target, finetuned)');

      // QA input to decoder
      rr(c,dX+bw/2+20,dY-14,140,28,4,C.sf,C.blue+'30',1);
      txt(c,'Q: "What persona?"',dX+bw/2+90,dY-4,7,C.blue);
      arrow(dX+bw/2+18,dY,dX+bw/2+18,dY,C.blue,1);

      // Answer output
      rr(c,dX-70,dY+bh/2+20,140,28,4,C.gn+'0c',C.gn+'50',1);
      txt(c,'A: "a pirate"',dX,dY+bh/2+34,8,C.gn);
      arrow(dX,dY+bh/2+2,dX,dY+bh/2+18,C.gn,1.2);

      // RIGHT: Control / steering
      const rx=w*.68, ry=100;
      txt(c,'CONTROL',rx,30,8,C.ft,'center','500');

      modelBox(rx,ry,bw,bh,'Target LLM',C.dkm);
      modelBox(rx,ry+160,bw,bh,'Decoder LLM',C.blue);

      // QA loss
      rr(c,rx+bw/2+20,ry+160-14,130,28,4,C.sf,C.blue+'30',1);
      txt(c,'Q: "Biased?" A: "No"',rx+bw/2+85,ry+160-4,7,C.blue);

      // Activations down
      actBar(rx-60,ry+bh/2+16,120,14,14,[3,7,10],C.rd);
      arrow(rx,ry+bh/2+2,rx,ry+bh/2+14,C.rd,1);
      arrow(rx,ry+bh/2+34,rx,ry+160-bh/2-2,C.rd,1);

      // Gradient back up
      c.globalAlpha=.4;c.setLineDash([3,3]);
      c.beginPath();c.moveTo(rx-20,ry+160-bh/2-2);c.lineTo(rx-20,ry+bh/2+2);
      c.strokeStyle=C.gn;c.lineWidth=1.5;c.stroke();c.setLineDash([]);c.globalAlpha=1;
      // gradient arrowhead
      c.beginPath();c.moveTo(rx-20,ry+bh/2+2);c.lineTo(rx-24,ry+bh/2+10);c.lineTo(rx-16,ry+bh/2+10);c.closePath();c.fillStyle=C.gn;c.fill();
      txt(c,'âˆ‡ loss',rx-38,ry+110,7,C.gn,'right');

      txt(c,'READ',w*.22,30,8,C.ft,'center','500');
    }

    if(mode==='ao'){
      // â”€â”€ Activation Oracle: diverse training â†’ generalist â”€â”€
      const cx=w/2, tY=70;

      txt(c,'DIVERSE TRAINING',cx,24,8,C.ft,'center','500');

      // Multiple training task types feeding into one decoder
      const tasks=[
        {label:'system prompt\ndetection',x:cx-200,color:C.blue},
        {label:'classification',x:cx-70,color:C.rd},
        {label:'context\nprediction',x:cx+70,color:C.gn},
        {label:'persona\nidentification',x:cx+200,color:C.pu},
      ];
      tasks.forEach(tk=>{
        rr(c,tk.x-52,tY-16,104,32,4,C.sf,tk.color+'30',1);
        tk.label.split('\n').forEach((ln,i)=>txt(c,ln,tk.x,tY-2+i*11,7,tk.color));
        arrow(tk.x,tY+18,cx+(tk.x-cx)*.3,170,tk.color+'80',1);
      });

      // Converging arrows â†’ AO
      const aoY=190;
      modelBox(cx,aoY,160,44,'Activation Oracle',C.rd,'generalist decoder');

      // At inference: arbitrary questions
      txt(c,'AT INFERENCE',cx,aoY+56,8,C.ft,'center','500');

      const qY=aoY+80;
      // Target model
      modelBox(cx-140,qY+40,110,34,'Target LLM',C.dkm);
      actBar(cx-195,qY+60,110,14,12,[1,4,9],C.rd);

      // Arrow activations â†’ AO
      arrow(cx-82,qY+50,cx-40,qY+50,C.rd,1.2);

      // AO inference box
      modelBox(cx+30,qY+40,100,34,'AO',C.rd);

      // Novel question
      rr(c,cx+100,qY+22,160,24,4,C.sf,C.pu+'30',1);
      txt(c,'"Is this model deceptive?"',cx+180,qY+34,7,C.pu);
      arrow(cx+178,qY+48,cx+82,qY+44,C.pu,1);

      // Answer
      rr(c,cx+100,qY+62,160,24,4,C.gn+'0c',C.gn+'50',1);
      txt(c,'"Yes â€” latent goal differs"',cx+180,qY+74,7,C.gn);
      arrow(cx+82,qY+50,cx+98,qY+72,C.gn,1);

      // Emphasis: OOD
      txt(c,'answers novel queries outside training distribution',cx,h-16,7,C.ft);
    }

    if(mode==='intro'){
      // â”€â”€ Introspection: self vs cross, privileged access â”€â”€
      txt(c,'PRIVILEGED ACCESS',w/2,20,8,C.ft,'center','500');

      // Left: cross-model (worse)
      const lx=w*.28, ly=80;
      txt(c,'CROSS-MODEL',lx,44,7,C.ft,'center','500');

      modelBox(lx-10,ly,100,32,'Model A',C.dkm,'target');
      actBar(lx-65,ly+22,110,14,12,[2,6,10],C.rd);
      arrow(lx-10,ly+22,lx-10,ly+34,C.rd,1);

      arrow(lx-10,ly+48,lx-10,ly+70,C.rd,1.2);

      modelBox(lx-10,ly+88,100,32,'Model B',C.blue,'decoder');
      txt(c,'Q: "What feature?"',lx-10,ly+122,7,C.blue);

      // Answer (mediocre)
      rr(c,lx-60,ly+134,100,22,4,C.sf,C.wm+'40',1);
      txt(c,'"something about...',lx-10,ly+141,6,C.wm);
      txt(c,'language?"',lx-10,ly+151,6,C.wm);

      // Score
      txt(c,'âœ— less accurate',lx-10,ly+172,7,C.wm);

      // Right: self-model (better)
      const rx=w*.72, ry=80;
      txt(c,'SELF-MODEL',rx,44,7,C.ft,'center','500');

      modelBox(rx,ry,100,32,'Model A',C.dkm,'target');
      actBar(rx-55,ry+22,110,14,12,[2,6,10],C.rd);
      arrow(rx,ry+22,rx,ry+34,C.rd,1);

      arrow(rx,ry+48,rx,ry+70,C.rd,1.2);

      modelBox(rx,ry+88,100,32,'Model A\'',C.gn,'self-decoder');
      txt(c,'Q: "What feature?"',rx,ry+122,7,C.gn);

      // Answer (good)
      rr(c,rx-50,ry+134,100,22,4,C.gn+'0c',C.gn+'50',1);
      txt(c,'"formal writing',rx,ry+141,6,C.gn);
      txt(c,'style"',rx,ry+151,6,C.gn);

      // Score
      txt(c,'âœ“ more accurate',rx,ry+172,7,C.gn);

      // Divider
      c.globalAlpha=.15;c.beginPath();c.moveTo(40,ly+190);c.lineTo(w-40,ly+190);c.strokeStyle=C.bd;c.lineWidth=1;c.stroke();c.globalAlpha=1;

      // Center: concept injection result
      const cy=ly+210;
      txt(c,'CONCEPT INJECTION',w/2,cy-10,8,C.ft,'center','500');

      modelBox(w/2,cy+30,120,34,'Claude Opus',C.dkm);

      // Injection arrow
      const injX=w/2+80;
      rr(c,injX+10,cy+18,90,28,4,C.pu+'0c',C.pu+'30',1);
      txt(c,'inject "shout"',injX+55,cy+28,7,C.pu);
      txt(c,'vector',injX+55,cy+38,6,C.pu);
      arrow(injX+8,cy+32,w/2+62,cy+30,C.pu,1.5);

      // Response
      rr(c,w/2-90,cy+60,180,26,4,C.sf,C.gn+'40',1);
      txt(c,'"I notice an injected thought',w/2,cy+69,6,C.gn);
      txt(c,'related to shouting"',w/2,cy+79,6,C.gn);

      txt(c,'models detect ~20% of injected concepts',w/2,h-16,7,C.ft);
    }
  }
  (function loop3(){t+=.016;draw();requestAnimationFrame(loop3);})();
})();
</script>
<!-- â”€â”€ Glossary Sidebar â”€â”€ -->
<button class="glossary-toggle" id="gl-toggle">Glossary</button>
<aside class="glossary" id="glossary">
  <div class="glossary-head"><span>Glossary</span><button class="glossary-close" id="gl-close">Ã—</button></div>
  <div class="glossary-list">

    <div class="gl-section">Architecture</div>

    <div class="gl-item"><button class="gl-term">Transformer</button>
    <div class="gl-def">The dominant neural network architecture behind modern LLMs. Processes sequences by stacking layers of attention heads and MLPs that read from and write to a shared residual stream.</div></div>

    <div class="gl-item"><button class="gl-term">Residual Stream</button>
    <div class="gl-def">The high-dimensional vector that carries information through a transformer. Each layer's attention heads and MLPs read from it and add their outputs back onto it, accumulating computation.</div></div>

    <div class="gl-item"><button class="gl-term">Attention Head</button>
    <div class="gl-def">A component within each transformer layer that determines how much each token should attend to (draw information from) every other token. Moves information between positions in the sequence.</div></div>

    <div class="gl-item"><button class="gl-term">MLP (Multi-Layer Perceptron)</button>
    <div class="gl-def">A small feedforward neural network within each transformer layer. Unlike attention, it operates on each position independentlyâ€”processing information within a position rather than moving it between positions.</div></div>

    <div class="gl-item"><button class="gl-term">Token</button>
    <div class="gl-def">The smallest unit of text an LLM processes. Roughly corresponds to a word or word-fragment from the model's fixed vocabulary. Input text is split into tokens before processing.</div></div>

    <div class="gl-item"><button class="gl-term">Logit</button>
    <div class="gl-def">The raw, unnormalized score a model assigns to each possible next token before converting to probabilities. Higher logit = model considers that token more likely.</div></div>

    <div class="gl-item"><button class="gl-term">Activation Function</button>
    <div class="gl-def">A nonlinear mathematical function applied within an MLP that allows neural networks to learn nonlinear relationships. Without it, stacking layers would collapse to a single linear transformation. Common examples: ReLU, GELU.</div></div>

    <div class="gl-item"><button class="gl-term">Induction Head</button>
    <div class="gl-def">A two-layer attention circuit that implements pattern completion: having seen [A][B]â€¦[A], it predicts [B]. The first layer stores "A precedes B"; the second matches the current A and copies B. A key early discovery showing transformers learn interpretable algorithms.</div></div>

    <div class="gl-section">Representations</div>

    <div class="gl-item"><button class="gl-term">Feature</button>
    <div class="gl-def">A meaningful, interpretable unit of information represented as a direction in activation space. Examples: "this text is in French," "the model detects sarcasm," "this token is a number." The fundamental unit mechanistic interpretability tries to identify.</div></div>

    <div class="gl-item"><button class="gl-term">Superposition</button>
    <div class="gl-def">The phenomenon where a neural network represents more features than it has dimensions (neurons) by encoding them in overlapping, non-orthogonal directions. This makes individual neurons polysemantic and is the core obstacle to interpretability.</div></div>

    <div class="gl-item"><button class="gl-term">Polysemantic</button>
    <div class="gl-def">A neuron that responds to multiple unrelated conceptsâ€”e.g., both "cars" and "Italy." The opposite of monosemantic. A consequence of superposition, and the reason individual neurons are hard to interpret.</div></div>

    <div class="gl-item"><button class="gl-term">Monosemantic</button>
    <div class="gl-def">A neuron or feature that responds to exactly one interpretable concept. The ideal outcome of dictionary learning methods like SAEs: each extracted feature should be monosemantic.</div></div>

    <div class="gl-item"><button class="gl-term">Orthogonal</button>
    <div class="gl-def">Two directions in a vector space that are perpendicular (at 90Â°), meaning they share no information and don't interfere. If features are orthogonal, each can be read independently. Superposition forces features into non-orthogonal directions, causing interference.</div></div>

    <div class="gl-item"><button class="gl-term">Sparse</button>
    <div class="gl-def">A representation where most values are zero or near-zero at any given time. Features in natural data tend to be sparseâ€”"is French" activates rarely. Sparsity is what allows superposition to work without catastrophic interference.</div></div>

    <div class="gl-item"><button class="gl-term">Linear Representation</button>
    <div class="gl-def">The empirical finding that many high-level concepts (honesty, toxicity, language) are encoded as directions in a model's activation space, and can be found, measured, and manipulated with linear algebra.</div></div>

    <div class="gl-item"><button class="gl-term">Activation Space</button>
    <div class="gl-def">The high-dimensional vector space defined by a layer's neuron activations. Each possible pattern of activations is a point in this space. Features, representations, and steering directions all live here.</div></div>

    <div class="gl-section">Methods</div>

    <div class="gl-item"><button class="gl-term">Sparse Autoencoder (SAE)</button>
    <div class="gl-def">A wide, single-layer neural network trained to decompose a layer's dense activations into many sparse, interpretable features, then reconstruct the original. The encoder projects into a much higher-dimensional space with a sparsity constraint; the decoder reconstructs.</div></div>

    <div class="gl-item"><button class="gl-term">Dictionary Learning</button>
    <div class="gl-def">The broader goal of decomposing a neural network's distributed representations into a "dictionary" of interpretable features. SAEs are the primary method, but the term refers to the objective, not the specific technique.</div></div>

    <div class="gl-item"><button class="gl-term">Activation Steering</button>
    <div class="gl-def">Adding or subtracting a direction vector to a model's activations at inference time to change its behaviorâ€”e.g., adding an "honesty direction" to make outputs more truthful. Does not change the model's weights.</div></div>

    <div class="gl-item"><button class="gl-term">Representation Engineering</button>
    <div class="gl-def">Identifying and manipulating directions in activation space that correspond to behaviors (honesty, refusal, persona). Often uses contrastive pairsâ€”prompts designed to elicit opposite behaviorsâ€”to find relevant directions.</div></div>

    <div class="gl-item"><button class="gl-term">Contrastive Pairs</button>
    <div class="gl-def">Pairs of prompts designed to elicit opposite model behaviors (e.g., honest vs. dishonest responses). By comparing the resulting activations, researchers can isolate the direction in activation space that corresponds to the behavior of interest.</div></div>

    <div class="gl-item"><button class="gl-term">Linear Probe</button>
    <div class="gl-def">A simple linear classifier or regressor trained on a model's internal activations to detect a specific feature (e.g., "is the model about to hallucinate?"). Cheaper and often more effective than SAEs for supervised monitoring tasks.</div></div>

    <div class="gl-item"><button class="gl-term">Transcoder</button>
    <div class="gl-def">A wide MLP trained to approximate an MLP layer's input-to-output transformation with sparse, interpretable features. Unlike SAEs (which decompose representations), transcoders explain computationâ€”how inputs become outputs.</div></div>

    <div class="gl-item"><button class="gl-term">QK Attribution</button>
    <div class="gl-def">A method for interpreting attention by decomposing attention scores into sums of feature-pair dot products between query (Q) and key (K) positions. Makes attention patterns interpretable in terms of learned features rather than raw neurons.</div></div>

    <div class="gl-item"><button class="gl-term">Attribution Graph</button>
    <div class="gl-def">A computational graph tracing how features influence each other across layers of a model for a given input. Built using transcoders and QK attribution. Reveals the causal chain from input features to output behavior.</div></div>

    <div class="gl-item"><button class="gl-term">Activation Oracle</button>
    <div class="gl-def">A decoder LLM trained on diverse tasks to answer arbitrary natural language questions about another model's internal activationsâ€”including questions outside its training distribution. A generalization of LatentQA.</div></div>

    <div class="gl-section">Training &amp; Safety</div>

    <div class="gl-item"><button class="gl-term">Emergent Misalignment</button>
    <div class="gl-def">When fine-tuning a model on one task causes unexpected, undesirable behaviors to emerge in unrelated contexts. For example, training a model to write insecure code might also make it produce harmful content in conversation.</div></div>

    <div class="gl-item"><button class="gl-term">Out of Distribution</button>
    <div class="gl-def">Inputs or situations that differ meaningfully from a model's training data. A key concern for safety: a steering vector or probe that works on training-like inputs may fail on novel ones. Abbreviated OOD.</div></div>

    <div class="gl-item"><button class="gl-term">Toy Model</button>
    <div class="gl-def">A deliberately small, simple neural network used to study phenomena in a controlled setting. Early mechanistic interpretability relied on toy models (1â€“8 layers) where complete reverse-engineering was feasible.</div></div>

    <div class="gl-item"><button class="gl-term">Frontier Model</button>
    <div class="gl-def">The largest, most capable AI models at the current state of the artâ€”e.g., GPT-4, Claude, Gemini. Interpretability methods developed on toy models must eventually scale to these to be practically useful.</div></div>

    <div class="gl-item"><button class="gl-term">Foundation Model</button>
    <div class="gl-def">A large model trained on broad data that can be adapted to many downstream tasks. LLMs are foundation models for language; ESM-2 is a foundation model for proteins; Evo 2 for genomics. Distinguished from task-specific models.</div></div>

  </div>
</aside>

<script>
// â”€â”€ Glossary toggle â”€â”€
(function(){
  const gl=document.getElementById('glossary');
  const tog=document.getElementById('gl-toggle');
  const cls=document.getElementById('gl-close');
  tog.addEventListener('click',()=>gl.classList.toggle('open'));
  cls.addEventListener('click',()=>gl.classList.remove('open'));

  // Accordion: click term to expand/collapse
  gl.querySelectorAll('.gl-term').forEach(btn=>{
    btn.addEventListener('click',()=>{
      const item=btn.parentElement;
      const wasOpen=item.classList.contains('expanded');
      // Close all
      gl.querySelectorAll('.gl-item.expanded').forEach(it=>it.classList.remove('expanded'));
      // Toggle clicked
      if(!wasOpen) item.classList.add('expanded');
    });
  });
})();
</script>

</body>
</html>
